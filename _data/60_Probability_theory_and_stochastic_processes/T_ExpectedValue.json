{
  "alternatePhrases": [
    "mean",
    "expectation value",
    "expectation"
  ],
  "definition": "Let us first consider a discrete random variable $X$ with values in\n$\\mathbb{R}$. Then $X$ has values in an at most countable set\n$\\mathcal{X}$. For $x\\in\\mathcal{X}$ denote the probability that $X=x$\nby $P_x$. If $$\\sum_{x\\in\\mathcal{X}}|x|P_x$$ converges, the sum\n$$\\sum_{x\\in\\mathcal{X}}xP_x$$ is well-defined. Its value is called the\n*expected value*, *expectation* or *mean* of $X$. It is usually denoted\nby $E(X)$.\n\nTaking this idea further, we can easily generalize to a continuous\nrandom variable $X$ with probability density $\\varrho$ by setting\n$$E(X)=\\int_{-\\infty}^\\infty x\\varrho(x)dx,$$ if this integral exists.\n\nFrom the above definition it is clear that the expectation is a linear\nfunction, i.e. for two random variables $X, Y$ we have\n$$E(aX+bY)=aE(X)+bE(Y)$$ for $a,b\\in\\mathbb{R}$.\n\nNote that the expectation does not always exist (if the corresponding\nsum or integral does not converge, the expectation does not exist. One\nexample of this situation is the Cauchy random variable).\n\nUsing the measure theoretical formulation of stochastics, we can give a\nmore formal definition. Let $(\\Omega, \\mathcal{A}, P)$ be a probability\nspace and $X:\\Omega\\to\\mathbb{R}$ a random variable. We now define\n$$E(X)=\\int_{\\Omega} XdP,$$ where the integral is understood as the\nLebesgue-integral with respect to the measure $P$.",
  "language": "INFORMAL",
  "phrase": "Expected Value",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/ExpectedValue"
    }
  ],
  "indexable": true
}