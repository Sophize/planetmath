{
  "language": "INFORMAL",
  "remarks": "",
  "statement": "Let $X(t)$, for $0 \\leq t \\leq T$, be a submartingale with continuous\nsample paths. Then for any constant $\\alpha > 0$,\n$${\\mathbb{P}}\\Bigl( \\max_{0 \\leq t \\leq T} X(t) \\geq \\alpha \\Bigr)\n\\leq \\frac{{\\mathbb{E}}[ X(T)^+ ]}{\\alpha}\\,.$$\n\n(The notation $X(T)^+$ means $\\max(X(T),0)$, the positive part of\n$X(T)$.)\n\nNotice the analogy with Markov\u2019s inequality. Of course, the conclusion\nis much stronger than Markov\u2019s inequality, as the probabilistic bound\napplies to an uncountable number of random variables. The continuity and\nsubmartingale hypotheses are used to establish the stronger bound.\n\nLet $\\{ t_i \\}_{i=1}^n$ be a partition of the interval $[0,T]$. Let\n$$B = \\Bigl\\{ \\max_{1 \\leq i \\leq n} X(t_i) \\geq \\alpha \\Bigr\\}$$ and\nsplit $B$ into disjoint parts $B_i$, defined by\n$$B_i = \\Bigl\\{ X(t_j) < \\alpha \\text{ for all } j < i\n                              \\text{ but } X(t_i) \\geq \\alpha \\Bigr\\}\\,.$$\nAlso let $\\{ {\\mathcal{F}}_t \\}$ be the filtration under which $X(t)$ is\na submartingale.\n\nThen $$\\begin{aligned}\n{\\mathbb{P}}(B) \n &=\n   \\sum_{i=1}^n {\\mathbb{E}}\\bigl[1(B_i)\\bigr] \\\\\n &\\leq \n  \\sum_{i=1}^n {\\mathbb{E}}\\left[ \\frac{X(t_i)}{\\alpha} \\, {\\mathbf{1}}(B_i) \\right] \n  & \\text{definition of $B_i$} \\\\\n &\\leq \n  \\frac{1}{\\alpha} \\sum_{i=1}^n \n    {\\mathbb{E}}\\Bigl[ {\\mathbb{E}}\\bigl[X(T) \\mid {\\mathcal{F}}_{t_i} \\bigr] \\, {\\mathbf{1}}(B_i) \\Bigr] \n  & \\text{$X(t)$ is submartingale} \\\\\n &=\n  \\frac{1}{\\alpha} \\sum_{i=1}^n\n    {\\mathbb{E}}\\Bigl[ {\\mathbb{E}}\\bigl[X(T) \\, {\\mathbf{1}}(B_i) \\mid {\\mathcal{F}}_{t_i} \\bigr] \\Bigr] \n  & \\text{$B_i$ is ${\\mathcal{F}}_{t_i}$-measurable} \\\\\n &=\n  \\frac{1}{\\alpha} \\sum_{i=1}^n\n    {\\mathbb{E}}\\bigl[ X(T) \\, {\\mathbf{1}}(B_i) \\bigr] \n  & \\text{iterated expectation} \\\\\n &=\n  \\frac{1}{\\alpha} {\\mathbb{E}}\\bigl[ X(T) \\, {\\mathbf{1}}(B) \\bigr] \\\\\n &\\leq\n  \\frac{1}{\\alpha} {\\mathbb{E}}\\bigl[ X(T)^+ \\, {\\mathbf{1}}(B) \\bigr] \\\\\n &\\leq\n  \\frac{1}{\\alpha} {\\mathbb{E}}\\bigl[ X(T)^+ \\bigr] &\n  \\text{monotonicity.} \\end{aligned}$$\n\nSince the sample paths are continuous by hypothesis, the event\n$$A = \\Bigl\\{ \\max_{0 \\leq t \\leq T} X(t) \\geq \\alpha \\Bigr\\}$$ can be\nexpressed as an countably infinite intersection of events of the form\n$B$ with finer and finer partitions $\\{ t_i \\}$ of the time interval\n$[0,T]$. By taking limits, it follows ${\\mathbb{P}}(A)$ has the same\nbound as the probabilities ${\\mathbb{P}}(B)$.\n\nLet $X(t)$, for $0 \\leq t \\leq T$, be a square-integrable martingale\npossessing continuous sample paths, whose unconditional mean is\n$m = {\\mathbb{E}}[X(0)]$. For any constant $\\alpha > 0$,\n$${\\mathbb{P}}\\Bigl( \\max_{0 \\leq t \\leq T} {\\lvertX(t)-m\\rvert} \\geq \\alpha \\Bigr)\n\\leq \\frac{\\operatorname{Var}[X(T)]}{\\alpha^2}\\,.$$\n\nApply Kolmogorov\u2019s martingale inequality to $(X(t)-m)^2$, which is a\nsubmartingale by Jensen\u2019s inequality.",
  "citations": [
    {
      "textCitation": "https://planetmath.org/KolmogorovsMartingaleInequality"
    }
  ],
  "indexable": true,
  "names": [
    "Kolmogorov's martingale inequality",
    "Kolmogorov's submartingale inequality"
  ]
}