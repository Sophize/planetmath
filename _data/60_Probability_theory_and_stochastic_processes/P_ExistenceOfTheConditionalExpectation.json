{
  "language": "INFORMAL",
  "remarks": "",
  "statement": "Let $(\\Omega,\\mathcal{F},\\mathbb{P})$ be a probability space and $X$ be\na random variable. For any $\\sigma$-algebra\n$\\mathcal{G}\\subseteq\\mathcal{F}$, we show the existence of the\nconditional expectation $\\mathbb{E}[X\\mid\\mathcal{G}]$. Although it is\npossible to do this using the Radon-Nikodym theorem, a different\napproach is used here which relies on the completeness of the vector\nspace $L^2$. The defining property of the conditional expectation\n$Y=\\mathbb{E}[X\\mid\\mathcal{G}]$ is $$\\label{cond exp}\n\\mathbb{E}[1_GY]=\\mathbb{E}[1_GX]$$ for sets $G\\in\\mathcal{G}$. We shall\nprove the existence of the conditional expectation for all nonnegative\nrandom variables and, more generally, whenever\n$\\mathbb{E}[|X|\\mid\\mathcal{G}]$ is almost surely finite.\n\nFirst, the conditional expectation of every square-integrable random\nvariable exists.\n\n\\[square integrable\\] Suppose that $\\mathbb{E}[X^2]<\\infty$. Then there\nis a $\\mathcal{G}$-measurable random variable $Y$ satisfying\n$\\mathbb{E}[Y^2]<\\infty$ and equation (\\[cond exp\\]) is satisfied for\nall $G\\in\\mathcal{G}$.\n\nConsider the norm $\\Vert Y\\Vert_2\\equiv\\mathbb{E}[Y^2]^{1/2}$ on the\nvector space $V=L^2(\\Omega,\\mathcal{F},\\mathbb{P})$ of real valued\nrandom variables $Y$ satisfying $\\mathbb{E}[Y^2]<\\infty$ (up to\n$\\mathbb{P}$ almost everywhere equivalence). This is given by the\nfollowing inner product\n$$\\langle Y_1,Y_2\\rangle\\equiv\\mathbb{E}[Y_1Y_2].$$ As $L^p$-spaces are\ncomplete, this makes $V$ into a Hilbert space (see also, ). Then,\n$U\\equiv L^2(\\Omega,\\mathcal{G},\\mathbb{P})$ is a complete, and hence\nclosed, subspace of $V$.\n\nBy the onto closed subspaces of Hilbert spaces, there is an orthogonal\nprojection $\\pi\\colon V\\rightarrow U$. In particular,\n$\\langle\\pi Y-Y,Z\\rangle=0$ for all $Y\\in V$ and $Z\\in U$. Setting\n$Y=\\pi X$ gives\n$$\\mathbb{E}[1_GY]-\\mathbb{E}[1_GX] = \\langle 1_G,\\pi X-X\\rangle=0$$ as\nrequired.\n\nWe can now prove the existence of conditional expectations of\nnonnegative random variables. Note that here there are no integrability\nconditions on $X$.\n\n\\[nonnegative\\] Let $X$ be a nonnegative random variable taking values\nin $\\mathbb{R}\\cup\\{\\infty\\}$. Then, there exists a nonnegative\n$\\mathcal{G}$-measurable random variable $Y$ taking values in\n$\\mathbb{R}\\cup\\{\\infty\\}$ and satisfying (\\[cond exp\\]) for all\n$G\\in\\mathcal{G}$. Furthermore, $Y$ is uniquely defined $\\mathbb{P}$-.\n\nFirst, let $X_n=\\min(n,X)$. As this is bounded, theorem \\[square\nintegrable\\] says that the conditional expectations\n$Y_n=\\mathbb{E}[Y_n\\mid\\mathcal{G}]$ exist. Furthermore, as $X_0=0$, we\nmay take $Y_0=0$. For any $n$, setting $G=\\{Y_{n+1}<Y_n\\}\\in\\mathcal{G}$\ngives\n$$\\mathbb{E}[1_G(Y_n-Y_{n+1})]=\\mathbb{E}[1_G(X_n-X_{n+1})]\\le 0.$$ So\n$1_G(Y_n-Y_{n+1})$ is a nonnegative random variable with nonpositive\nexpectation, hence is almost surely equal to zero. Therefore,\n$Y_{n+1}\\ge Y_n$ (almost surely) and, by replacing $Y_n$ with the\nmaximum of $Y_1,\\ldots\\,Y_n$ we may suppose that $(Y_n)$ is an\nincreasing sequence of random variables. Setting $Y=\\sup_nY_n$, the\nmonotone convergence theorem gives\n$$\\mathbb{E}[1_GY]=\\lim_{n\\rightarrow\\infty}\\mathbb{E}[1_GY_n]=\\lim_{n\\rightarrow\\infty}\\mathbb{E}[1_GX_n]=\\mathbb{E}[1_GX]$$\nas required.\n\nFinally, suppose that $\\tilde Y$ is also a nonnegative\n$\\mathcal{G}$-measurable random variable satisfying (\\[cond exp\\]). For\nany $x\\in\\mathbb{R}$, setting $G=\\{\\tilde Y>Y,x>Y\\}$ then $1_GY$ is\nbounded and,\n$$\\mathbb{E}[1_G(\\tilde Y-Y)]=\\mathbb{E}[1_GX]-\\mathbb{E}[1_GX]=0$$\nshowing that $\\mathbb{P}(G)=0$. Letting $x$ increase to infinity gives\n$\\tilde Y\\le Y$ (almost surely) and, similarly, $Y\\le \\tilde Y$ so that\n$Y=\\tilde Y$ almost surely.\n\nFinally, we show existence of the conditional expectation of every\nrandom variable $X$ satisfying $\\mathbb{E}[|X|\\mid\\mathcal{G}]<\\infty$\nalmost surely. Note, in particular, that this is satisfied whenever $X$\nis integrable, as\n$$\\mathbb{E}[\\mathbb{E}[|X|\\mid\\mathcal{G}]]=\\mathbb{E}[|X|]<\\infty.$$\n\nLet $X$ be a random variable such that\n$\\mathbb{E}[|X|\\mid\\mathcal{G}]<\\infty$ almost surely. Then, there\nexists a $\\mathcal{G}$-measurable random variable $Y$ such that\n$\\mathbb{E}[1_G|Y|]<\\infty$ and (\\[cond exp\\]) is satisfied for every\n$G\\in\\mathcal{G}$ with $\\mathbb{E}[1_G|X|]<\\infty$.\n\nFurthermore, $Y$ is uniquely defined up to $\\mathbb{P}$-a.e.\nequivalence.\n\nThe positive and negative parts $X_+,X_-$ of $X$ satisfy\n$$\\mathbb{E}[X_+\\mid\\mathcal{G}]+\\mathbb{E}[X_-\\mid\\mathcal{G}]=\\mathbb{E}[|X|\\mid\\mathcal{G}]<\\infty$$\nalmost surely. We can therefore set\n$Y_{\\pm}\\equiv\\mathbb{E}[X_\\pm\\mid\\mathcal{G}]$ and $Y=Y_+-Y_-$.\n\nIf $G\\in\\mathcal{G}$ satisfies $\\mathbb{E}[1_G|X|]<\\infty$ then\n$\\mathbb{E}[1_GY_\\pm]=\\mathbb{E}[1_GX_\\pm]<\\infty$, so\n$\\mathbb{E}[1_G|Y|]<\\infty$ and,\n$$\\mathbb{E}[1_GY]=\\mathbb{E}[1_GY_+]-\\mathbb{E}[1_GY_-]=\\mathbb{E}[1_GX_+]-\\mathbb{E}[1_GX_-]=\\mathbb{E}[1_GX]$$\nas required.\n\nFinally, suppose that $\\tilde Y$ satisfies the same conditions as $Y$.\nFor any $x\\ge 0$ set $G=\\{Y_++Y_-\\le x,\\tilde Y> Y\\}\\in\\mathcal{G}$.\nThen, $$\\mathbb{E}[1_G|X|]=\\mathbb{E}[1_G(Y_++Y_-)]\\le x<\\infty.$$ So,\n$\\mathbb{E}[1_G|Y|]$ and $\\mathbb{E}[1_G|\\tilde Y|]$ are finite, hence\n(\\[cond exp\\]) gives\n$$\\mathbb{E}[1_G(\\tilde Y - Y)]=\\mathbb{E}[1_GX]-\\mathbb{E}[1_GX]=0.$$\nSo $\\mathbb{P}(G)=0$ and, letting $x$ increase to infinity,\n$\\tilde Y\\le Y$ almost surely. Similarly, $Y\\le\\tilde Y$ and therefore\n$\\tilde Y=Y$ almost surely.",
  "citations": [
    {
      "textCitation": "https://planetmath.org/ExistenceOfTheConditionalExpectation"
    }
  ],
  "indexable": true,
  "names": [
    "existence of the conditional expectation"
  ]
}