{
  "alternatePhrases": [
    "joint characteristic function"
  ],
  "definition": "Let $X$ be a random variable. The *characteristic function* of $X$ is a\nfunction $\\varphi_X:\\mathbb{R}\\rightarrow\\mathbb{C}$ defined by\n$$\\varphi_X(t) = Ee^{itX} = E\\cos(tX) + iE\\sin(tX),$$ that is,\n$\\varphi_X(t)$ is the expectation of the random variable $e^{itX}$.\n\nGiven a random vector $\\overline{X}=(X_1,\\dots,X_n)$, the characteristic\nfunction of $\\overline{X}$, also called *joint characteristic function*\nof $X_1,\\dots,X_n$, is a function\n$\\varphi_{\\overline{X}}:\\mathbb{R}^n\\rightarrow\\mathbb{C}$ defined by\n$$\\varphi_{\\overline{X}}(t)=Ee^{i{\\overline{t}}\\cdot{\\overline{X}}},$$\nwhere $\\overline{t} = (t_1,\\dots,t_n)$ and\n${\\overline{t}}\\cdot{\\overline{X}} = t_1X_1+\\cdots+t_nX_n$ (the dot\nproduct.)\n\n**Remark.** If $F_X$ is the distribution function associated to $X$, by\nthe properties of expectation we have\n$$\\varphi_X(t) = \\int_\\mathbb{R} e^{itx}dF_X(x),$$ which is known as the\nFourier-Stieltjes transform of $F_X$, and provides an alternate\ndefinition of the characteristic function. From this, it is clear that\nthe characteristic function depends only on the distribution function of\n$X$, hence one can define the characteristic function associated to a\ndistribution even when there is no random variable involved. This\nimplies that two random variables with the same distribution must have\nthe same characteristic function. It is also true that each\ncharacteristic function determines a unique distribution; hence the ,\nsince it characterizes the distribution function (see property 6.)\n\n**Properties**\n\n1.  The characteristic function is bounded by $1$, i.e.\n    $|\\varphi_X(t)|\\leq 1$ for all $t$;\n\n2.  $\\varphi_X(0)=1$;\n\n3.  $\\overline{\\varphi_X(t)} = \\varphi_X(-t)$, where $\\overline{z}$\n    denotes the complex conjugate of $z$;\n\n4.  $\\varphi_X$ is uniformly continuous in $\\mathbb{R}$;\n\n5.  If $X$ and $Y$ are independent random variables, then\n    $\\varphi_{X+Y} = \\varphi_X\\varphi_Y$;\n\n6.  The characteristic function determines the distribution function;\n    hence, $\\varphi_X=\\varphi_Y$ if and only if $F_X = F_Y$. This is a\n    consequence of the *inversion* : Given a random variable $X$ with\n    characteristic function $\\varphi$ and distribution function $F$, if\n    $x$ and $y$ are continuity points of $F$ such that $x<y$, then\n    $$F(x) - F(y) = \\frac{1}{2\\pi}\\lim_{s\\rightarrow\\infty}\\int_{-s}^s\n    \\frac{e^{-itx}-e^{-ity}}{it}\\varphi(t)dt;$$\n\n7.  A random variable $X$ has a symmetrical distribution (i.e. one such\n    that $F_X = F_{-X}$) if and only if $\\varphi_X(t)\\in \\mathbb{R}$ for\n    all $t\\in \\mathbb{R}$;\n\n8.  For real numbers $a,b$, $\\varphi_{aX+b}(t) = e^{itb}\\varphi_X(at)$;\n\n9.  If $E|X|^n<\\infty$, then $\\varphi_X$ has continuous $n$-th\n    derivatives and\n    $$\\frac{d^k\\varphi_X}{dt^k}(t) = \\varphi_X^{(k)}(t)=\\int_\\mathbb{R}(ix)^ke^{itx}dF_X(x), \\;\\; 1\\leq k\\leq n.$$\n    Particularly, $\\varphi_X^{(k)}(0)=i^kEX^k$; characteristic functions\n    are similar to moment generating functions in this sense.\n\nSimilar properties hold for joint characteristic functions. Other\nimportant result related to characteristic functions is the Paul L\u00e9vy\ncontinuity theorem.",
  "language": "INFORMAL",
  "phrase": "Characteristic Function",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/CharacteristicFunction1"
    }
  ],
  "indexable": true
}