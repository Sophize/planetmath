{
  "argumentText": "Let $h(x)$ be the step function ($h(x)=1$ for\u00a0$x\\geq 0\n$, $h(x)=0$ for $x<0$); then, by generalized Markov inequality, for any\n$t > 0$ and any $\\varepsilon \\geq 0$, $$\\begin{aligned}\n\\Pr\\left\\{ \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) >\\varepsilon \\right\\} \n&=&E\\left[ h\\left( \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) -\\varepsilon\n\\right) \\right] \\leq  \\\\\n&\\leq &E\\left[ e^{t\\left( \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right)\n-\\varepsilon \\right) }\\right] = \\\\\n&=&\\exp (-\\varepsilon t)E\\left[ e^{\\sum_{i=1}^{n}t\\left(\nX_{i}-E[X_{i}]\\right) }\\right] = \\\\\n&=&\\exp (-\\varepsilon t)E\\left[ \\prod_{i=1}^{n}e^{t\\left(\nX_{i}-E[X_{i}]\\right) }\\right] = \\\\\n\\text{(by independence)} &=&\\exp (-\\varepsilon t)\\prod_{i=1}^{n}E\\left[\ne^{t\\left( X_{i}-E[X_{i}]\\right) }\\right] = \\\\\n&=&\\exp \\left( -\\varepsilon t+\\sum_{i=1}^{n}\\ln E\\left[ e^{t\\left(\nX_{i}-E[X_{i}]\\right) }\\right] \\right) = \\\\\n&=&\\exp \\left[ -\\left( t\\varepsilon -\\psi (t)\\right) \\right]. \\end{aligned}$$\n\nSince this expression is valid for any $t > 0$, the best bound is\nobtained taking the supremum:\n\n$$\\Pr\\left\\{ \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) >\\varepsilon \\right\\}\n\\leq e^{-\\sup_{t > 0}\\left( t\\varepsilon -\\psi (t)\\right)}$$\n\nwhich proves part c).\n\nTo prove part a), let\u2019s observe that\n$\\Psi (0)=\\sup_{t > 0}(-\\psi (t))=-\\inf_{t > 0}(\\psi (t))$ and that\n$$\\begin{aligned}\nE\\left[ e^{t\\left( X_{i}-E[X_{i}]\\right) }\\right]  &\\geq &E[1+t\\left(\nX_{i}-E[X_{i}]\\right) ]= \\\\\n&=&E[1]+tE[X_{i}]-tE[E[X_{i}]]= \\\\\n&=&1=E\\left[ e^{t\\left( X_{i}-E[X_{i}]\\right) }\\right] _{t=0}\\end{aligned}$$\nthat is, $t=0$ is the infimum point for $E\\left[ e^{t\\left(\nX_{i}-E[X_{i}]\\right) }\\right] $ $\\forall i$ and consequently for $\\psi\n(t)=\\sum_{i=1}^{n}\\ln E\\left[ e^{t\\left( X_{i}-EX_{i}\\right) }\\right] $,\nso as a conclusion $\\Psi (0)=-\\psi(0)=0$\n\nb\\) Let $x>0$ be fixed and let $t_{0}$ be the supremum point for\n$tx-\\psi (t)\n$; we have to show that $t_{0}x-\\psi (t_{0})>0$.\n\nBy differentiation, $\\psi ^{\\prime }(t_{0})=x$.\n\nLet\u2019s recall that the moment generating function is convex, so $\\psi\n^{\\prime \\prime }(t)>0$. Writing the Taylor expansion for $\\psi (t)$\naround $t=t_{0}$, we have, with a suitable $t_1<t_{0}$,\n$$0=\\psi (0)=\\psi (t_{0})-\\psi ^{\\prime }(t_{0})t_{0}+\\frac{1}{2}\\psi ^{\\prime\n\\prime }(t_{1})t_{0}^{2}$$ that is\n$$\\Psi (x)=t_{0}x-\\psi (t_{0})=t_{0}\\psi ^{\\prime }(t_{0})-\\psi (t_{0})=\\frac{1%\n}{2}\\psi ^{\\prime \\prime }(t_{1})t_{0}^{2}>0$$\n\nThe convexity of $\\Psi (x)$ follows from the fact that $\\Psi (x)$ is the\nsupremum of the linear (and hence convex) functions ${tx-\\psi (t)}$ and\nso must be convex itself.\n\nEventually, in to prove that $ \\Psi (x)$ is an increasing function,\nlet\u2019s note that\n$$\\Psi ^{\\prime }(0)=\\lim_{x\\rightarrow 0}\\frac{\\Psi (x)-\\Psi (0)}{x}%\n=\\lim_{x\\rightarrow 0}\\frac{\\Psi (x)}{x}>0$$ and that, by Taylor formula\nwith Lagrange form remainder, for a $\\xi=\\xi(x)$\n$$\\Psi ^{\\prime }(x)=\\Psi ^{\\prime }(0)+\\Psi ^{\\prime \\prime }(\\xi )x\\geq 0$$\nsince $\\Psi ^{\\prime \\prime }(\\xi )\\geq 0$ by convexity and $x\\geq 0$ by\nhypotheses.",
  "conclusion": "#P_ChernoffCramerBound",
  "language": "INFORMAL",
  "premises": [
    "#P_planetmath_ZFC"
  ],
  "citations": [
    {
      "textCitation": "https://planetmath.org/ProofOfChernoffCramerBound"
    }
  ],
  "indexable": false,
  "names": [
    "proof of Chernoff-Cramer bound"
  ]
}