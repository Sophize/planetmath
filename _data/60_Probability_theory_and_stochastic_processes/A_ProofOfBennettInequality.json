{
  "argumentText": "By , we have:\n$$\\Pr\\left\\{ \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) >\\varepsilon \\right\\}\n\\leq \\exp \\left[ -\\sup_{t\\geq 0}\\left( t\\varepsilon -\\psi (t)\\right) \\right]$$\nwhere\n$$\\psi (t)=\\sum_{i=1}^{n}\\left( \\ln E\\left[ e^{tX_{i}}\\right] -tE\\left[ X_{i}%\n\\right] \\right).$$ Keeping in mind that the condition\n$$\\Pr\\left\\{ \\left\\vert X_{i}\\right\\vert \\leq M\\right\\} =1\\text{ \\ }\\forall i$$\nimplies that, for all $i$,\n$$E[\\left\\vert X_{i}\\right\\vert ^{k}]\\leq M^{k} \\text{ \\ }\\forall k\\geq 0$$\n(see for a proof) and since $\\ln x\\leq x-1$ $\\forall x>0$,\nand$$E[\\left\\vert X\\right\\vert ^{k}]\\leq M^{k}\\text{ \\ }\\Longrightarrow \\text{ \\ }%\nE\\left[ \\left\\vert X\\right\\vert ^{k}\\right] \\leq E\\left[ X^{2}\\right] M^{k-2}%\n\\text{ \\ \\ \\ \\ \\ \\ \\ }\\forall k\\geq 2,k\\in N$$ (see for a proof), one\nhas: $$\\begin{aligned}\n\\psi (t) &=&\\sum_{i=1}^{n}\\left( \\ln E\\left[ e^{tX_{i}}\\right] -tE\\left[\nX_{i}\\right] \\right) \\\\\n&\\leq &\\sum_{i=1}^{n}E\\left[ e^{tX_{i}}\\right] -tE\\left[ X_{i}\\right] -1 \\\\\n&=&\\sum_{i=1}^{n}E\\left[ \\sum_{k=0}^{\\infty }\\frac{\\left( tX_{i}\\right) ^{k}%\n}{k!}\\right] -tE\\left[ X_{i}\\right] -1 \\\\\n&=&\\sum_{i=1}^{n}\\left( \\sum_{k=0}^{\\infty }\\frac{t^{k}E\\left[ X_{i}^{k}%\n\\right] }{k!}\\right) -tE\\left[ X_{i}\\right] -1 \\\\\n&=&\\sum_{i=1}^{n}\\left( \\sum_{k=2}^{\\infty }\\frac{t^{k}E\\left[ X_{i}^{k}%\n\\right] }{k!}\\right)  \\\\\n&\\leq &\\sum_{i=1}^{n}\\left( \\sum_{k=2}^{\\infty }\\frac{t^{k}E\\left[\n\\left\\vert X_{i}\\right\\vert ^{k}\\right] }{k!}\\right)  \\\\\n&\\leq &\\sum_{i=1}^{n}\\left( \\sum_{k=2}^{\\infty }\\frac{t^{k}E\\left[ X_{i}^{2}%\n\\right] M^{k-2}}{k!}\\right) \\\\\n&=&\\sum_{k=2}^{\\infty }\\frac{t^{k}M^{k-2}\\sum_{i=1}^{n}E\\left[ X_{i}^{2}%\n\\right] }{k!} \\\\\n&=&\\frac{v^{2}}{M^{2}}\\sum_{k=2}^{\\infty }\\frac{\\left( tM\\right) ^{k}}{k!} \\\\\n&=&\\frac{v^{2}}{M^{2}}\\left[ \\exp \\left( tM\\right) -tM-1\\right] \\end{aligned}$$\n\nOne can now write\n$$\\sup_{t\\geq 0}\\left( t\\varepsilon -\\psi (t)\\right) \\geq \\sup_{t\\geq 0}\\left(\nt\\varepsilon -\\frac{v^{2}}{M^{2}}\\left( e^{tM}-tM-1\\right) \\right)\n=\\sup_{t>0}\\left[ \\frac{v^{2}}{M^{2}}\\left( \\frac{M^{2}\\varepsilon }{v^{2}}%\nt-\\left( e^{tM}-tM-1\\right) \\right) \\right].$$\n\nBy elementary calculus, we obtain the value of $t$ that maximizes the\nexpression in round brackets:\n$$t_{opt}=\\frac{1}{M}\\ln \\left( 1+\\frac{M\\varepsilon }{v^{2}}\\right)$$\nwhich, once plugged into the bound, yields\n$$\\Pr\\left\\{ \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) >\\varepsilon \\right\\}\n\\leq \\exp \\left[ -\\frac{v^{2}}{M^{2}}\\left( \\left( 1+\\frac{M\\varepsilon }{%\nv^{2}}\\right) \\ln \\left( 1+\\frac{M\\varepsilon }{v^{2}}\\right) -\\frac{%\nM\\varepsilon }{v^{2}}\\right) \\right].$$\n\nObserving that\n$\\left( 1+x\\right) \\ln \\left( 1+x\\right) -x\\geq \\frac{x}{2}%\n\\ln \\left( 1+x\\right) $ $\\forall x\\geq 0$ (see ), one gets the\nsub-optimal yet more easily manageable formula:\n$$\\Pr\\left\\{ \\sum_{i=1}^{n}\\left( X_{i}-E[X_{i}]\\right) >\\varepsilon \\right\\}\n\\leq \\exp \\left[ -\\frac{\\varepsilon }{2M}\\ln \\left( 1+\\frac{\\varepsilon M}{%\nv^{2}}\\right) \\right].$$",
  "conclusion": "#P_BennettInequality",
  "language": "INFORMAL",
  "premises": [
    "#P_planetmath_ZFC"
  ],
  "citations": [
    {
      "textCitation": "https://planetmath.org/ProofOfBennettInequality"
    }
  ],
  "indexable": false,
  "names": [
    "proof of Bennett inequality"
  ]
}