{
  "alternatePhrases": [
    "Huffman encoding"
  ],
  "definition": "*Huffman coding* is a method of lossless data compression, and a form of\nentropy encoding. The basic idea is to map an alphabet to a\nrepresentation for that alphabet, composed of strings of variable size,\nso that symbols that have a higher probability of occurring have a\nsmaller representation than those that occur less often.\n\nThe key to Huffman coding is Huffman\u2019s algorithm, which constructs an\nextended binary tree of minimum weighted path length from a list of\nweights. For this problem, our list of weights consists of the\nprobabilities of symbol occurrence. From this tree (which we will call a\n*Huffman tree* for convenience), the mapping to our variable-sized\nrepresentations can be defined.\n\nThe mapping is obtained by the path from the root of the Huffman tree to\nthe leaf associated with a symbol\u2019s weight. The method can be arbitrary,\nbut typically a value of 0 is associated with an edge to any left child\nand a value of 1 with an edge to any right child (or vice-versa). By\nconcatenating the labels associated with the edges that make up the path\nfrom the root to a leaf, we get a binary string. Thus the mapping is\ndefined.\n\nIn order to recover the symbols that make up a string from its\nrepresentation after encoding, an inverse mapping must be possible. It\nis important that this mapping is unambiguous. We can show that all\npossible strings formed by concatenating any number of path labels in a\nHuffman tree are indeed unambiguous, due to the fact that it is a\ncomplete binary tree. That is, given a string composed of Huffman codes,\nthere is exactly one possible way to decompose it into the individual\ncodes.\n\nAmbiguity occurs if there is any path to some symbol whose label is a\nprefix of the label of a path to some other symbol. In the Huffman tree,\nevery symbol is a leaf. Thus it is impossible for the label of a path to\na leaf to be a prefix of any other path label, and so the mapping\ndefined by Huffman coding has an inverse and decoding is possible.\n\n### Example {#example .unnumbered}\n\nFor a simple example, we will take a short phrase and derive our\nprobabilities from a frequency count of letters within that phrase. The\nresulting encoding should be good for compressing this phrase, but of\ncourse will be inappropriate for other phrases with a different letter\ndistribution.\n\nWe will use the phrase \u201cmath for the people by the people\u201d. The\nfrequency count of characters in this phrase are as follows (let\n$\\textvisiblespace$ denote the spaces).\n\n       **Letter**        **Count**\n  --------------------- -----------\n   $\\textvisiblespace$       6\n            e                6\n            p                4\n            h                3\n            o                3\n            t                3\n            l                2\n            a                1\n            b                1\n            f                1\n            m                1\n            r                1\n            y                1\n        **Total**           33\n\nWe will simply let the frequency counts be the weights. If we pair each\nsymbol with its weight, and pass this list of weights to Huffman\u2019s\nalgorithm, we will get something like the following tree (edge labels\nhave been added).\n\n![image](tree.11)\n\nFrom this tree we obtain the following mapping.\n\n       **Letter**        **Count**   **Huffman code**   **Weight**\n  --------------------- ----------- ------------------ ------------\n   $\\textvisiblespace$       6             111              18\n            e                6              01              12\n            p                4             101              12\n            h                3             1100             12\n            o                3             1101             12\n            t                3             001              9\n            l                2             0001             8\n            a                1            00000             5\n            b                1            00001             5\n            f                1            10000             5\n            m                1            10001             5\n            r                1            10010             5\n            y                1            10011             5\n        **Total**           33              -              113\n\nIf we were to use a fixed-sized encoding, our original string would have\nto be 132 bits in length. This is because there are 13 symbols,\nrequiring 4 bits of representation, and the length of our string is 33.\n\nThe weighted path length of this Huffman tree is 113. Since these\nweights came directly from the frequency count of our string, the number\nof bits required to represent our string using this encoding is the same\nas the weight of 113. Thus the Huffman encoded string is $85\\%$ the\nlength of the fixed-sized encoding. Arithmetic encoding can in most\ncases obtain even greater compression, although it is not quite as\nsimple to implement.",
  "language": "INFORMAL",
  "phrase": "Huffman Coding",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/HuffmanCoding"
    }
  ],
  "indexable": true
}