{
  "alternatePhrases": [],
  "definition": "The general eigenvalue problem {#the-general-eigenvalue-problem .unnumbered}\n==============================\n\nSuppose we have a vector space $V$ and a linear operator\n$A\\in\\operatorname{End}(V)$. Then the eigenvalue problem is this:\n\nFor what values $\\lambda$ does the equation $$Ax = \\lambda x$$ have a\nnonzero solution $x$? For such a $\\lambda$, what are all the solution\nvectors $x$?\n\nValues $\\lambda$ admitting a solution are called eigenvalues; nonzero\nsolutions $x$ are called eigenvectors.\n\nThe question may be rephrased as a question about the linear operator\n$(A-\\lambda I)$, where $I$ is the identity on $V$. Since $\\lambda I$ is\ninvertible whenever $\\lambda$ is nonzero, one might expect that\n$(A-\\lambda I)$ should be invertible for \u201cmost\u201d $\\lambda$. As usual,\nwhen dealing with infinite-dimensional spaces, the situation is more\ncomplicated.\n\nA special sitation arises when $V$ has an inner product under which $A$\nis self-adjoint. In this case, $A$ has a discrete set of eigenvalues,\nand if $x_{\\lambda_1}$ and $x_{\\lambda_2}$ are eigenvectors\ncorresponding to distinct eigenvalues, then $x_{\\lambda_1}$ and\n$x_{\\lambda_2}$ are orthogonal. In fact, since the inner product makes\n$V$ into a normed linear space one can find an orthonormal basis for $V$\nconsisting entirely of eigenvectors of $A$.\n\nDifferential eigenvalue problems {#differential-eigenvalue-problems .unnumbered}\n================================\n\nMany problems in physics and elsewhere lead to differential eigenvalue\nproblems, that is, problems where the vector space is some space of\ndifferentiable functions and where the linear operator involves\nmultiplication by functions and taking derivatives. Such problems arise\nfrom the method of separation of variables, for example. One class of\neigenvalue problems that is well-studied are Sturm-Liouville problems,\nwhich always lead to self-adjoint operators. The sequences of\neigenvectors obtained are therefore orthogonal under a suitable inner\nproduct.\n\nAn example of a Sturm-Liouville problem is this: Find a function $f(x)$\nsatisfying $$f''(x) = -\\lambda f(x)$$ and $$f(0)=f(1)=0.$$ Observe that\nfor most values of $\\lambda$, there is only the solution $f(x)=0$. If\n$\\lambda=(n\\pi)^2$ for some $n$, though, $\\sin(\\sqrt{\\lambda}x)$ is a\nsolution. Observe that if $n\\neq m$, then\n$$\\int_0^1 \\sin(n\\pi x)\\sin(m\\pi x) dx = 0.$$ Moreover, recalling the\nproperties of Fourier series, we see that any function satisfying the\nboundary conditions can be written as an infinite linear combination of\neigenvectors of this problem.\n\nMany of the families of special functions that turn up throughout\napplied mathematics do so precisely because they are an orthogonal\nfamily of eigenvectors for a Sturm-Liouville problem. For example, the\ntrigonometric functions sine and cosine and the Bessel functions both\narise in this way.\n\nMatrix eigenvalue problems {#matrix-eigenvalue-problems .unnumbered}\n==========================\n\nMatrix eigenvalue problems arise in a number of different situations.\nThe eigenvalues of a matrix describe its behaviour in a\ncoordinate-independent way; theorems about diagonalization allow\ncomputation of matrix powers efficiently, for example. As a result,\nmatrix eigenvalues are useful in statistics, for example in analyzing\nMarkov chains and in the fundamental theorem of demography.\n\nMatrix eigenvalue problems also arise as the discretization of\ndifferential eigenvalue problems.\n\nAn example of where a matrix eigenvalue problem arises is the\ndetermination of the main axes of a second order surface $Q=x^TAx=1$\n(defined by a symmetric matrix $A$). The task is to find the places\nwhere the normal\n$${\\nabla}(Q) = \\left(\\frac{\\partial Q}{\\partial x_1}, \\cdots , \\frac{\\partial Q}{\\partial x_n}\\right) = 2 A x$$\nis parallel to the vector $x$, i.e $Ax=\\lambda x$.\n\nA solution $x$ of the above equation with $x^TAx=1$ has the squared\ndistance $x^Tx=d^2$ from the origin. Therefore, $\\lambda x^Tx =1$ and\n$d^2=1/\\lambda$. The main axes are\n$a_i = 1/\\sqrt{\\lambda_i}\\quad (i=1,\\ldots,n)$.\n\nThe matrix eigenvalue problem can be written as $(A-\\lambda I)x=0$. A\nnon-trivial solution to this system of $n$ linear homogeneous equations\nexists if and only if the determinant $$\\det(A-\\lambda I) = \n\\left|\\begin{matrix}\n a_{11}-\\lambda & a_{12} & \\cdots & a_{1n} \\\\\n a_{21} & a_{22}-\\lambda & \\cdots & a_{2n} \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n a_{n1} & a_{n2} & \\cdots & a_{nn}-\\lambda\n\\end{matrix} \\right| = 0$$\n\nThis $n$th degree polynomial in $\\lambda$ is called the characteristic\npolynomial. Its roots $\\lambda$ are called the eigenvalues and the\ncorresponding vectors $x$ eigenvectors. In the example, $x$ is a right\neigenvector for $\\lambda$; a left eigenvector $y$ is defined by\n$y^TA=\\mu y^T$.\n\nNumerical eigenvalue problems {#numerical-eigenvalue-problems .unnumbered}\n=============================\n\nFrequently, one wishes to solve the eigenvalue problem approximately\n(generally on a computer). While one can do this using generic matrix\nmethods such as Gaussian elimination, $LU$ factorization, and others,\nthese have problems due to roundoff error when attempting to deal with\neigenvalue problems. Other methods are necessary. For example, a\n$QR$-based method is a much more adequate tool (\\[Golub89\\]); it works\nas follows. Assume that $A\\in\\mathbb R^{n\\times n}$ is diagonalizable.\nThe $QR$ iteration is given by\n\n> $A_0 = A$\\\n> for $k = 1,2,\\ldots$\\\n> $\\quad A_k =: Q_kR_k$\\\n> $\\quad A_{k+1} := R_kQ_k$\\\n> end\n\nAt each step, the matrix $Q_k$ is orthogonal and $R_k$ is upper\ntriangular.\n\nNote that $$A_{k+1} = \\big(Q_0\\cdots Q_k)^{\\mathrm T}AQ_0\\cdots Q_k.$$\nFor a full matrix, the $QR$ iteration requires $O(n^3)$ flops per step.\nThis is prohibitively expensive, so we first reduce $A$ to an upper\nHessenberg matrix, $H$, using an orthogonal similarity transformation:\n$$U^{\\mathrm T}AU = H$$ ($H$ is upper Hessenberg if $h_{ij} = 0$ for\n$i>j+1$). We will use Householder transformations to achieve this. Note\nthat if $A$ is symmetric then $H$ is symmetric, and hence tridiagonal.\n\nThe eigenvalues of $A$ are found by applying iteratively the $QR$\ndecomposition to $H$. These two matrices have the same eigenvalues as\nthey are similar. In particular: $H = H_1$ is decomposed into\n$H_1 = Q_1R_1$, then an $H_2$ is computed, $H_2 = R_1Q_1$. $H_2$ is\nsimilar to $H_1$ because $H_2 = R_1Q_1 = Q_1^{-1}H_1Q_1$, and is\ndecomposed to $H_2 = Q_2R_2$. Then $H_3$ is formed, $H_3 = R_2Q_2$, etc.\nIn this way a sequence of $H_i$\u2019s (with the same eigenvalues) is\ngenerated, that finally converges to (for conditions, see \\[Golub89\\])\n$$\\begin{pmatrix}\n \\lambda_1 & * & * & \\cdots & * & * \\\\\n 0 & \\lambda_2 & * & \\cdots & * & * \\\\\n 0 & 0 & \\lambda_3 & \\cdots & * & * \\\\\n \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n 0 & 0 & 0 & \\cdots & \\lambda_{n-1} & * \\\\\n 0 & 0 & 0 & \\cdots & 0 & \\lambda_n \n\\end{pmatrix}$$ for the Hessenberg and $$\\begin{pmatrix}\n \\lambda_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n 0 & \\lambda_2 & 0 & \\cdots & 0 & 0 \\\\\n 0 & 0 & \\lambda_3 & \\cdots & 0 & 0 \\\\\n \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n 0 & 0 & 0 & \\cdots & \\lambda_{n-1} & 0 \\\\\n 0 & 0 & 0 & \\cdots & 0 & \\lambda_n\n\\end{pmatrix}$$ for the tridiagonal.\n\nReferences {#references .unnumbered}\n==========\n\nDAB\n\n:   Originally from The Data Analysis Briefbook ()\n\nGolub89\n\n:   Gene H. Golub and Charles F. van Loan: Matrix Computations, 2nd\n    edn., The John Hopkins University Press, 1989.",
  "language": "INFORMAL",
  "phrase": "Eigenvalue Problem",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/EigenvalueProblem"
    }
  ],
  "indexable": true
}