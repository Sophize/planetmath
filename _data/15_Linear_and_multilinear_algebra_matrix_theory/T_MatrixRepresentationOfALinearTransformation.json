{
  "alternatePhrases": [
    "ordered bases",
    "standard ordered bases"
  ],
  "definition": "Linear transformations and matrices are the two most fundamental notions\nin the study of linear algebra. The two concepts are intimately related.\nIn this article, we will see how the two are related. We assume that all\nvector spaces are finite dimensional and all vectors are written as\ncolumn vectors.\n\n### Linear transformations as matrices {#linear-transformations-as-matrices .unnumbered}\n\nLet $V,W$ be vector spaces (over a common field $k$) of dimension $n$\nand $m$ respectively. Fix bases $A=\\lbrace v_1,\\ldots, v_n\\rbrace$ and\n$B=\\lbrace w_1,\\ldots, w_m\\rbrace$ for $V$ and $W$ respectively. We\nshall order these bases so that $v_i < v_j$ and $w_i < w_j$ whenever\n$i < j$. To distinguish an ordinary set from an ordered set, we shall\nadopt the notation $\\langle v_1,\\ldots, v_n\\rangle$ to mean the set\n$\\lbrace v_1,\\ldots, v_n\\rbrace$ with ordering $v_i\\le v_j$ whenever\n$i\\le j$. The importance of ordering these bases will be apparent\nshortly.\n\nFor any linear transformation $T:V\\to W$, we can write\n$$T(v_j)=\\sum_{i=1}^m \\alpha_{ij} w_i$$ for each\n$j\\in \\lbrace 1,\\ldots, n\\rbrace$ and $\\alpha_{ij}\\in k$. We define the\n*matrix associated with the linear transformation $T$ and ordered bases\n$A,B$* by $$[T]^A_B:=(\\alpha_{ij}),$$ where $1\\le i\\le n$ and\n$1\\le j\\le m$. $[T]^A_B$ is a $m\\times n$ matrix whose entries are in\n$k$. When $A=B$, we often write $[T]_A:=[T]^A_A$. In addition, when both\nordered bases are standard bases $E_n,E_m$ ordered in the obvious way,\nwe write $[T]:=[T]^{E_n}_{E_m}$.\n\n**Examples.**\n\n1.  Let $T:\\mathbb{R}^3\\to \\mathbb{R}^4$ be given by\n    $$T\\begin{pmatrix}x\\\\y\\\\z\\end{pmatrix}=\\begin{pmatrix}x+2y+z \\\\ z \\\\ -x+y-5z \\\\ 3x+2z\\end{pmatrix}.$$\n    Using the standard ordered bases\n    $$E_3=\\Bigg\\langle \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}\\Bigg\\rangle \\mbox{ for }\\mathbb{R}^3 \\quad \\mbox{ and } \\quad E_4=\\Bigg\\langle \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}, \\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}\\Bigg\\rangle \\mbox{ for }\\mathbb{R}^4$$\n    ordered in the obvious way. Then,\n    $$T\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\-1\\\\3\\end{pmatrix},\\quad T\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}=\\begin{pmatrix}2\\\\0\\\\1\\\\0\\end{pmatrix},\\quad T\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}=\\begin{pmatrix}1\\\\1\\\\-5\\\\2\\end{pmatrix},$$\n    so the matrix $[T]^{E_3}_{E_4}$ associated with $T$ and the standard\n    ordered bases $E_3$ and $E_4$ is the $4\\times 3$ matrix\n    $$\\begin{pmatrix} 1&2&1 \\\\ 0&0&1 \\\\ -1&1&-5 \\\\ 3&0&2 \\end{pmatrix}.$$\n\n2.  Let $T$ be the same linear transformation as above. However, let\n    $E'_3$ be the same basis as $E_3$ except that the order is reversed:\n    $e_3<e_2<e_1$. Then\n    $$[T]^{E'_3}_{E_4}=\\begin{pmatrix} 1&2&1 \\\\ 1&0&0 \\\\ -5&1&-1 \\\\ 2&0&3 \\end{pmatrix}.$$\n    Note that this matrix is just the matrix from the previous example\n    except that the first and the last columns have been switched.\n\n3.  Again, let $T$ be the same as before. Now, let $E'_4$ be the ordered\n    basis whose elements are those of $E_4$ but the order is now given\n    by $e_2<e_1<e_4<e_3$. Then\n    $$[T]^{E'_3}_{E'_4}=\\begin{pmatrix} 1&0&0 \\\\ 1&2&1 \\\\ 2&0&3 \\\\ -5&1&-1 \\end{pmatrix}.$$\n    Note that this matrix is just the matrix from the previous example\n    except that the first two rows and the last two rows have been\n    interchanged.\n\n**Remarks**.\n\n-   From the examples above, we note several important features of a\n    matrix representation of a linear transformation:\n\n    1.  the matrix depends on the bases given to the vector spaces\n\n    2.  the ordering of a basis is important\n\n    3.  switching the order of a given basis amounts to switching\n        columns and rows of the matrix, essentially multiplying a matrix\n        by a permutation matrix.\n\n-   Some basic properties of matrix representations of linear\n    transformations are\n\n    1.  If $T:V\\to W$ is a linear transformation, then\n        $[rT]^A_B=r[T]^A_B$, where $A,B$ are ordered bases for $V,W$\n        respectively.\n\n    2.  If $S,T:V\\to W$ are linear transformations, then\n        $[S+T]^A_B=[S]^A_B+[T]^A_B$, where $A$ and $B$ are ordered bases\n        for $V$ and $W$ respectively.\n\n    3.  If $S:U\\to V$ and $T:V\\to W$, then $[TS]^A_C=[T]^B_C[S]^A_B$,\n        where $A,B,C$ are ordered bases for $U,V,W$ respectively.\n\n    4.  As a result, $T$ is invertible iff $[T]^A_B$ is an invertible\n        matrix iff $\\dim(V)=\\dim(W)$.\n\n-   We could have represented all vectors as row vectors. However, doing\n    so would mean that the matrix representation $M_1$ of a linear\n    transformation $T$ would be the transpose of the matrix\n    representation $M_2$ of $T$ if the vectors were represented as\n    column vectors: $M_1=M_2^T$, and that the application of the\n    matrices to vectors would be from the right of the vectors:\n    $$\\begin{pmatrix}a&b&c\\end{pmatrix}\\begin{pmatrix}1&0&-1&3\\\\2&0&1&0\\\\1&1&-5&2\\end{pmatrix}\\qquad \\mbox{ instead of } \\qquad\\begin{pmatrix} 1&2&1 \\\\ 0&0&1 \\\\ -1&1&-5 \\\\ 3&0&2 \\end{pmatrix}\\begin{pmatrix}a\\\\b\\\\c\\end{pmatrix}.$$\n\n### Matrices as linear transformations {#matrices-as-linear-transformations .unnumbered}\n\nEvery $m\\times n$ matrix $A$ over a field $k$ can be thought of as a\nlinear transformation from $k^n$ to $k^m$ if we view each vector\n$v\\in k^n$ as a $n\\times 1$ matrix (a column) and the mapping is done by\nthe matrix multiplication $Av$, which is a $m\\times 1$ matrix (a column\nvector in $k^m$). Specifically, we define $T_A:k^n\\to k^m$ by\n$$T_A(v):=Av.$$ It is easy to see that $T_A$ is indeed a linear\ntransformation. Furthermore, $[T_A]=[T_A]^{E_n}_{E_m}=A$, since the\nrepresentation of vectors as $n$-tuples of elements in $k$ is the same\nas expressing each vector under the standard basis (ordered) in the\nvector space $k^n$. Below we list some of the basic properties:\n\n1.  $T_{rA}=rT_A$, for any $r\\in k$,\n\n2.  $T_A+T_B=T_{A+B}$, where $A,B$ are $m\\times n$ matrices over $k$\n\n3.  $T_A\\circ T_B=T_{AB}$, where $A$ is an $m\\times n$ matrix and $B$ is\n    an $n\\times p$ matrix over $k$\n\n4.  $T_A$ is invertible iff $A$ is an invertible matrix.\n\n**Remark**. As we can see from the discussion above, if we fix sets of\nbase elements for a vector space $V$ and $W$, there is a one-to-one\ncorrespondence between the set of matrices (of the same size) over the\nunderlying field $k$ and the set of linear transformations from $V$ to\n$W$.\n\n[3]{} Friedberg, Insell, Spence. [*Linear Algebra*]{}. Prentice-Hall\nInc., 1997.",
  "language": "INFORMAL",
  "phrase": "Matrix Representation Of A Linear Transformation",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/MatrixRepresentationOfALinearTransformation"
    }
  ],
  "indexable": true
}