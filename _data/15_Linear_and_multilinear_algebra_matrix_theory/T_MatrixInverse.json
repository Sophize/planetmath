{
  "alternatePhrases": [
    "inverse",
    "matrix inversion"
  ],
  "definition": "Matrix Inverse\n==============\n\nThe *inverse* of an $n \\times n$ matrix $A$ is denoted by $A^{-1}$. The\ninverse is defined so that\n\n$$A A^{-1} = A^{-1}A = I_n$$\n\nwhere $I_n$ is the $n \\times n$ identity matrix.\n\nIt should be stressed that **only** square matrices have inverses\nproper\u2013 however, a matrix of any size may have \u201cleft\u201d and \u201cright\u201d\ninverses (which will not be discussed here).\n\nA precondition for the existence of the matrix inverse $A^{-1}$ (i.e.\nthe matrix is *invertible*) is that $\\det A \\ne 0$ (the determinant is\nnonzero), the reason for which we will see in a second.\n\nThe general form of the inverse of a matrix $A$ is\n\n$$A^{-1} = \\frac{1}{\\det(A)} \\operatorname{adj}(A)$$\n\nwhere $\\operatorname{adj}(A)$ is the *adjugate* of $A$ (the matrix\nformed by the cofactors of $A$, i.e. with\n$\\operatorname{adj}(A)_{ij} = C_{ij}(A)$).[^1] This can also be thought\nof as a generalization of the $2 \\times 2$ formula given in the next\nsection. However, due to the inclusion of the determinant in the\nexpression, it is impractical to actually use this to calculate\ninverses.\n\nThis general form also explains why the determinant must be nonzero for\ninvertibility; as we are dividing through by its value.\n\nAn invertible matrix is also said to be nonsingular.\n\nCalculating the Inverse By Hand\n===============================\n\n**Method 1:**\n\nAn easy way to calculate the inverse of a matrix by hand is to form an\naugmented matrix $[A|I]$ from $A$ and $I_n$, then use Gaussian\nelimination to transform the left half into $I$. At the end of this\nprocedure, the right half of the augmented matrix will be $A^{-1}$ (that\nis, you will be left with $[I|A^{-1}]$).\n\n**Method 2:**\n\nOne can calculate the $i,j$th element of the inverse by using the\ngeneral formula; i.e.\n\n$$A^{-1}_{ji} = C_{ij}(A) / \\det{A}$$\n\nwhere $C_{ij}(A)$ is the $i, j$th cofactor expansion of the matrix $A$.\n\nNote that the indices on the left-hand side are swapped relative to the\nright-hand side.\n\n2-by-2 case:\n------------\n\nFor the $2\\times 2$ case, the general formula reduces to a memorable\nshortcut. For the $2\\times 2$ matrix\n\n$$M = \\begin{bmatrix}a & b \\\\ c & d \\end{bmatrix}$$\n\nThe inverse $M^{-1}$ is always\n\n$$M^{-1} = \\left(\\frac{1}{\\det M}\\right) \\begin{bmatrix}d & -b \\\\ -c & a\\end{bmatrix}$$\n\nwhere $\\det M$ is simply $ad - bc$.\n\nRemarks\n-------\n\nSome caveats: computing the matrix inverse for ill-conditioned matrices\nis error-prone; special care must be taken and there are sometimes\nspecial algorithms to calculate the inverse of certain classes of\nmatrices (for example, Hilbert matrices).\n\nAvoiding the Inverse and Numerical Calculation\n==============================================\n\nThe need to find the matrix inverse depends on the situation\u2013 whether\ndone by hand or by computer, and whether the matrix is simply a part of\nsome equation or expression or not.\n\nInstead of computing the matrix $A^{-1}$ as part of an equation or\nexpression, it is nearly always better to use a matrix factorization\ninstead. For example, when solving the system $Ax = b$, actually\ncalculating $A^{-1}$ to get $x=A^{-1}b$ is discouraged. LU-factorization\nis typically used instead.\n\nWe can even use this fact to speed up our calculation of the inverse by\nitself. We can cast the problem as finding $X$ in\n\n$$AX = B$$\n\nFor $n \\times n$ matrices $A$, $X$, and $B$ (where $X = A^{-1}$ and\n$B = I_n$). To solve this, we first find the $LU$ decomposition of $A$,\nthen iterate over the columns, solving $ Ly = Pb_k $ and $ Ux_k = y $\neach time ($k = 1 \\ldots n$). The resulting values for $x_k$ are then\nthe columns of $A^{-1}$.\n\nElements of Invertible Matrices\n===============================\n\nTypically the matrix elements are members of a field when we are\nspeaking of inverses (i.e. the reals, the complex numbers). However, the\nmatrix inverse may exist in the case of the elements being members of a\ncommutative ring, provided that the determinant of the matrix is a unit\nin the ring.\n\n[3]{}\n\nGolub and Van Loan, \u201cMatrix Computations,\u201d Johns Hopkins Univ. Press,\n1996.\n\n\u201cMatrix Math,\u201d\n\n[^1]: Some other sources call the adjugate the adjoint; however on PM\n    the adjoint is reserved for the conjugate transpose.",
  "language": "INFORMAL",
  "phrase": "Matrix Inverse",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/MatrixInverse"
    }
  ],
  "indexable": true
}