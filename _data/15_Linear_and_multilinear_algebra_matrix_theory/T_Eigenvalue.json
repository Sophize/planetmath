{
  "alternatePhrases": [],
  "definition": "Let $V$ be a vector space over a field $k$, and let $A$ be an\nendomorphism of $V$ (meaning a linear mapping of $V$ into itself). A\nscalar $\\lambda\\in k$ is said to be an *eigenvalue* of $A$ if there is a\nnonzero $x \\in V$ for which $$Ax = \\lambda x\\;.$$ Geometrically, one\nthinks of a vector whose direction is unchanged by the action of $A$,\nbut whose magnitude is multiplied by $\\lambda$.\n\nIf $V$ is finite dimensional, elementary linear algebra shows that there\nare several equivalent definitions of an eigenvalue:\n\n\\(2) The linear mapping $$B=\\lambda I - A$$ i.e.\n$B:x\\mapsto \\lambda x-Ax$, has no inverse.\n\n\\(3) $B$ is not injective.\n\n\\(4) $B$ is not surjective.\n\n\\(5) $\\det(B)=0$, i.e. $\\det(\\lambda I-A)=0$.\n\nBut if $V$ is of infinite dimension, (5) has no meaning and the\nconditions (2) and (4) are not equivalent to (1). A scalar $\\lambda$\nsatisfying (2) (called a *spectral value* of $A$) need not be an\neigenvalue. Consider for example the complex vector space $V$ of all\nsequences $(x_n)_{n=1}^{\\infty}$ of complex numbers with the obvious\noperations, and the map $A: V \\to V$ given by\n$$A(x_1, x_2, x_3, \\dots) = (0, x_1, x_2, x_3, \\dots) \\;.$$ Zero is a\nspectral value of $A$, but clearly not an eigenvalue.\n\nNow suppose again that $V$ is of finite dimension, say $n$. The function\n$$\\chi(\\lambda)=\\det(B)$$ is a polynomial of degree $n$ over $k$ in the\nvariable $\\lambda$, called the *characteristic polynomial* of the\nendomorphism $A$. (Note that some writers define the characteristic\npolynomial as $\\det(A-\\lambda I)$ rather than $\\det(\\lambda I-A)$, but\nthe two have the same zeros.)\n\nIf $k$ is ${\\mathbb{C}}$ or any other algebraically closed field, or if\n$k={\\mathbb{R}}$ and $n$ is odd, then $\\chi$ has at least one zero,\nmeaning that $A$ has at least one eigenvalue. In no case does $A$ have\nmore than $n$ eigenvalues.\n\nAlthough we didn\u2019t need to do so here, one can compute the coefficients\nof $\\chi$ by introducing a basis of $V$ and the corresponding matrix for\n$B$. Unfortunately, computing $n \\times n$ determinants and finding\nroots of polynomials of degree $n$ are computationally messy procedures\nfor even moderately large $n$, so for most practical purposes variations\non this naive scheme are needed. See the eigenvalue problem for more\ninformation.\n\nIf $k={\\mathbb{C}}$ but the coefficients of $\\chi$ are real (and in\nparticular if $V$ has a basis for which the matrix of $A$ has only real\nentries), then the non-real eigenvalues of $A$ appear in conjugate\npairs. For example, if $n=2$ and, for some basis, $A$ has the matrix\n$$A = \\begin{pmatrix} 0 & -1 \\\\ 1 & 0 \\end{pmatrix}$$ then\n$\\chi(\\lambda)=\\lambda^2+1$, with the two zeros $\\pm i$.\n\nEigenvalues are of relatively little importance in connection with an\ninfinite-dimensional vector space, unless that space is endowed with\nsome additional structure, typically that of a Banach space or Hilbert\nspace. But in those cases the notion is of great value in physics,\nengineering, and mathematics proper. Look for \u201cspectral theory\u201d for more\non that subject.",
  "language": "INFORMAL",
  "phrase": "Eigenvalue",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/Eigenvalue"
    }
  ],
  "indexable": true
}