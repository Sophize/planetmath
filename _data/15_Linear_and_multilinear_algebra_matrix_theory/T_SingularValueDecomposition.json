{
  "alternatePhrases": [
    "SVD",
    "singular value",
    "singular vector"
  ],
  "definition": "Any real $m \\times n$ matrix $A$ can be decomposed into\n\n$$A = USV^T$$\n\nwhere $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$\northogonal matrix, and $S$ is a unique $m \\times n$ diagonal matrix with\nreal, non-negative elements $\\sigma_i$, $i = 1, \\ldots , \\min(m,n)$ , in\ndescending order:\n\n$$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(m,n)} \\ge 0$$\n\nThe $\\sigma_i$ are the *singular values* of $A$ and the first\n$\\min(m,n)$ columns of $U$ and $V$ are the left and right (respectively)\n*singular vectors* of $A$. $S$ has the form:\n\n$$\\begin{bmatrix}\\Sigma \\\\ 0\\end{bmatrix} \\operatorname{if} m \\ge n \\;\\operatorname{and}\\; \\begin{bmatrix}\\Sigma & 0 \\end{bmatrix} \\operatorname{if} m < n,$$\n\nwhere $\\Sigma$ is a diagonal matrix with the diagonal elements\n$\\sigma_1,\\sigma_2,\\ldots , \\sigma_{\\min(m,n)}$. We assume now\n$m \\ge n$. If $r=\\operatorname{rank}(A) < n $ , then\n\n$$\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > \\sigma_{r+1} = \\cdots = \\sigma_n = 0.$$\n\nIf $\\sigma_r \\ne 0$ and $\\sigma_{r+1} = \\cdots = \\sigma_n = 0$, then $r$\nis the rank of $A$. In this case, $S$ becomes an $r \\times r$ matrix,\nand $U$ and $V$ shrink accordingly. SVD can thus be used for rank\ndetermination.\n\nThe SVD provides a numerically robust solution to the least-squares\nproblem. The matrix-algebraic phrasing of the least-squares solution $x$\nis\n\n$$x = (A^T A)^{-1} A^T b$$\n\nThen utilizing the SVD by making the replacement $A=USV^T$ we have\n\n$$x = V \\begin{bmatrix} \\Sigma^{-1} & 0 \\end{bmatrix} U^T b .$$\n\n[**References**]{}\n\n-   Originally from The Data Analysis Briefbook ()",
  "language": "INFORMAL",
  "phrase": "Singular Value Decomposition",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/SingularValueDecomposition"
    }
  ],
  "indexable": true
}