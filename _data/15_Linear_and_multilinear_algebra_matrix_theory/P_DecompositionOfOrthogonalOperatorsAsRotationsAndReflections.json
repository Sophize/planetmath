{
  "language": "INFORMAL",
  "remarks": "",
  "statement": "Let $V$ be a $n$-dimensional real inner product space\n($0 < n < \\infty$). Then every orthogonal operator $T$ on $V$ can be\ndecomposed into a series of two-dimensional rotations and\none-dimensional reflections on mutually orthogonal subspaces of $V$.\n\nWe first explain the general idea behind the proof. Consider a rotation\n$R$ of angle $\\theta$ in a two-dimensional space. From its orthonormal\nbasis representation $$\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\; \\cos \\theta\n\\end{bmatrix}\\,,$$ we find that the characteristic polynomial of $R$ is\n$t^2 - 2 \\cos \\theta + 1$, with (complex) roots $e^{\\pm i\\theta}$. (Not\nsurprising, since multiplication by $e^{i\\theta}$ in the complex plane\nis rotation by $\\theta$.) Thus given the characteristic polynomial of\n$R$, we can almost recover[^1] its rotation angle.\n\nIn the case of a reflection $S$, the eigenvalues of $S$ are $-1$ and\n$1$; so again, the characteristic polynomial of $S$ will provide some\ninformation on $S$.\n\nSo in $n$ dimensions, we are also going to look at the complex\neigenvalues and eigenvectors of $T$ to recover information about the\nrotations represented by $T$.\n\nBut there is one technical point \u2014 $T$ is a transformation on a real\nvector space, so it does not really have \u201ccomplex eigenvalues and\neigenvectors\u201d. To make this concept rigorous, we must consider the\ncomplexification ${T^\\mathbb{C}}$ of $T$, the operator defined by\n${T^\\mathbb{C}}(x+iy) = Tx + iTy$ in the vector space ${V^\\mathbb{C}}$\nconsisting of elements of the form $x+iy$, for $x, y \\in V$. (For more\ndetails, see the entry on .)\n\nFor any linear operator $T\\colon V \\to V$, there exists a one- or\ntwo-dimensional subspace $W$ which is invariant under $T$.\n\nConsider ${T^\\mathbb{C}}$ and its characteristic polynomial. By the\nFundamental Theorem of Algebra, the characteristic polynomial has a\ncomplex root $\\lambda = \\alpha + i \\beta$. Then there is an eigenvector\n$x+iy \\neq 0$ with eigenvalue $\\lambda$. We have\n$$Tx + iTy = {T^\\mathbb{C}}(x+iy) = \\lambda(x + iy) = (\\alpha x - \\beta y) + i(\\beta x + \\alpha y)\\,.$$\nEquating real and imaginary components, we see that $$\\begin{aligned}\nTx &= \\alpha x - \\beta y \\in W \\\\\nTy &= \\beta x + \\alpha y \\in W\\end{aligned}$$ where\n$W = \\operatorname{span}\\{ x, y\\}$. $W$ is two-dimensional if $x$ and\n$y$ are linearly independent; otherwise it is one-dimensional[^2]. And\nwe have $T(W) \\subseteq W$ as claimed.\n\nWe recursively factor $T$; formally, the proof will be by induction on\nthe dimension $n$.\n\nThe case $n=1$ is trivial. We have $\\det T = \\pm 1$; if $\\det T = 1$\nthen $T$ is the identity; otherwise $Tx = -x$ is a reflection.\n\nFor larger $n$, by Lemma 1 there exists a $T$-invariant subspace $W$.\nThe orthogonal complement $W^\\perp$ of $W$ is $T$-invariant also,\nbecause for all $x \\in W^\\perp$ and $y \\in W$, $$\\begin{aligned}\n\\langle Tx, y \\rangle &= \\langle x, T^{-1} y \\rangle & \\textrm{because $T$ preserves inner product} \\\\\n&= 0 & \\textrm{because $T^{-1}(W) = W$.}\\end{aligned}$$ Let $T_W$ be the\noperator that acts as $T$ on $W$ and is the identity on $W^\\perp$.\nSimilarly, let $T_{W^\\perp}$ be the operator that acts as $T$ on\n$W^{\\perp}$ and is the identity on $W$. Then\n$T = T_W \\circ T_{W^\\perp}$. $T_W$ restricted to $W$ is orthogonal, and\nsince $W$ is one- or two-dimensional, $T_W$ must therefore be a rotation\nor reflection (or the identity) in a line or plane.\n\n$T_{W^\\perp}$ restricted to $W^\\perp$ is also orthogonal. $W^\\perp$ has\ndimension $<n$, so by the induction hypothesis we can continue to factor\nit into operators acting on subspaces of $W^\\perp$ that are mutually\northogonal. These subspaces will of course also be orthogonal to $W$.\n\nThe proof is now complete, except that we did not rule out $T_W$ being a\nreflection even when $W$ is two-dimensional. But of course, if $T_W$ is\na reflection in two dimensions, then it can be factored as a reflection\non a one-dimensional subspace $\\operatorname{span}\\{v\\}$ composed with\nthe identity on $\\operatorname{span}\\{ v \\}^\\perp$.\n\nActually, in the third paragraph of the proof, we implicitly assumed\nthat every orthogonal operator on two-dimensional is either a rotation\nor reflection. This is a well-known fact, but it does need to be\nformally proven:\n\nIf $V$ is a two-dimensional real inner product space, then every\northogonal operator $T$ is either a rotation or reflection.\n\nFix any orthonormal basis $\\{ e_1, e_2 \\}$ for $V$. Since $T$ is\northogonal, ${\\lVertTe_1\\rVert} = {\\lVerte_1\\rVert} = 1$, i.e. $Te_1$ is\na unit vector on the plane, so there exists an angle $\\theta$ (unique\nmodulo $2\\pi$) such that\n$Te_1 = \\cos \\theta \\, e_1 + \\sin \\theta \\, e_2$. Similarly $Te_2$ is a\nunit vector, but since $e_1$ and $e_2$ are orthogonal, so are $Te_1$ and\n$Te_2$. Then it is found that the solution to $Te_2$ must be either\n$-\\sin \\theta \\,  e_1 + \\cos \\theta \\, e_2$ or\n$\\sin \\theta \\, e_1 - \\cos \\theta \\, e_2$. Putting all this together,\nthe matrix for $T$ is: $$\\begin{bmatrix}\n\\cos \\theta & \\mp\\sin \\theta \\\\\n\\sin \\theta & \\pm \\cos \\theta\n\\end{bmatrix}\\,.$$ The first solution for $Te_2$ corresponds to a\nrotation matrix (and $\\det T = 1$); the second solution for $Te_2$\ncorresponds to a (and $\\det T = -1$).\n\nRemarks\n=======\n\ni.  Observe that the two equations for $Tx$ and $Ty$ appearing in the\n    proof of Lemma 1 do specify a rotation of angle $\\pm \\theta$ when\n    $\\lambda = \\alpha + i\\beta = e^{i\\theta}$ and $x, y$ are\n    orthonormal. So by examining the complex eigenvales and eigenvectors\n    of ${T^\\mathbb{C}}$, we can reconstruct the rotation.\n\n    This construction can be used to give an alternate proof of Theorem\n    1; we sketch it below:\n\n    The complexified space ${V^\\mathbb{C}}$ has an inner product\n    structure inherited from $V$ (again, see for details). Let\n    $U = {T^\\mathbb{C}}$. Since $T$ is orthogonal, $U$ is unitary, and\n    hence normal ($U^* U = UU^*$). There exists an orthonormal basis of\n    eigenvectors for $U$ (the ). Let $z = x+iy$ is any one of these\n    eigenvectors with a complex, non-real eigenvalue $\\lambda$. Then\n    ${\\lvert\\lambda\\rvert} = 1$ because $U$ is unitary, and\n    $\\overline{z} = x-iy$ is another eigenvector with eigenvalue\n    $\\overline{\\lambda}$. Using the ${V^\\mathbb{C}}$ inner product\n    formula, the vectors $x/\\sqrt{2}$ and $y/\\sqrt{2}$ can be shown to\n    be orthonormal. Then the proof of Lemma 1 shows that $T$ acts as a\n    rotation in the plane $\\operatorname{span}\\{ x, y \\}$. All such\n    planes obtained will be orthogonal to each other.\n\n    To summarize, the orthogonal subplanes of rotation are found by\n    grouping conjugate pairs of complex eigenvectors. If one actually\n    needs to determine the planes of rotation explicitly (for dimensions\n    $n \\geq 4$), then probably it is better to work directly with the\n    complexified matrix, rather than to factor the matrix over the\n    reals.\n\nii. The decomposition of $T$ is not unique. However, it is always\n    possible to obtain a decomposition which contains at most one\n    reflection, because any two single-dimensional reflections can\n    always be combined into a two-dimensional rotation. In any case, the\n    parity of the number of reflections in a decomposition of $T$ is\n    invariant, because the parity is equal to $\\det T$.\n\niii. In the decomposition, the component rotations and reflections all\n    commute because they act on orthogonal subspaces.\n\niv. If we take a basis for $V$ describing the mutually orthogonal\n    subspaces in Theorem 1, the matrix for $T$ looks like:\n    $$\\begin{bmatrix}\n    \\cos \\theta_1 & -\\sin \\theta_1 \\\\\n    \\sin \\theta_1 & \\; \\cos \\theta_1 \\\\\n    & & \\ddots \\\\\n    & & & \\cos \\theta_k & -\\sin \\theta_k \\\\\n    & & & \\sin \\theta_k & \\; \\cos \\theta_k \\\\\n    & & & & & \\pm 1 \\\\\n    & & & & & & +1 \\\\\n    & & & & & & & \\ddots \\\\\n    & & & & & & & & +1\n    \\end{bmatrix}$$ where $\\theta_1, \\dotsc, \\theta_k$ are the rotation\n    angles, one for each orthogonal subplane, and the $\\pm 1$ in the\n    middle is the reflective component (if present). The rest of the\n    entries in the matrix are zero.\n\nv.  Sometimes any orthogonal operator $T$ with $\\det T = 1$ is called a\n    rotation, even though strictly speaking it is actually series of\n    rotations (each on different \u201caxes\u201d). Similarly, when $\\det T = -1$,\n    $T$ may be called a reflection, even though again it is not always a\n    single (one-dimensional) reflection.\n\n    In this language, a rotation composed with a rotation will always be\n    a rotation; a rotation composed with a reflection is a reflection;\n    and two reflections composed together will always be a rotation.\n\nvi. In ${\\mathbb{R}}^3$, an orthogonal operator with positive\n    determinant is necessarily a rotation on one axis which is left\n    fixed (except when the operator is the identity). This follows\n    simply because there is no way to fit more than one orthogonal\n    subplane into three-dimensional space.\n\n    A composition of two rotations in ${\\mathbb{R}}^3$ would then be a\n    rotation too. On the other hand it is not at all obvious what\n    relation the axis of rotation of the composition has with the\n    original two axes of rotation.\n\n    For an explicit formula for a rotation matrix in ${\\mathbb{R}}^3$\n    that does not require manual calculation of the basis vectors for\n    the rotation subplane, see Rodrigues\u2019 rotation formula.\n\nvii. In ${\\mathbb{R}}^n$, reflections can be carried out by first\n    embedding ${\\mathbb{R}}^n$ into ${\\mathbb{R}}^{n+1}$ and then\n    rotating ${\\mathbb{R}}^{n+1}$. (Here, the words \u201crotation\u201d and\n    \u201creflection\u201d being taken in their extended sense of (v).) For\n    example, in the plane, a right hand can be rotated in\n    ${\\mathbb{R}}^3$ into a left hand.\n\n    To be specific, suppose we embed ${\\mathbb{R}}^n$ in\n    ${\\mathbb{R}}^{n+1}$ as the first $n$ coordinates. Then we gain an\n    extra degree of freedom in the last coordinate of\n    ${\\mathbb{R}}^{n+1}$ (with coordinate vector $e_{n+1}$). Given an\n    orthogonal operator $T\\colon {\\mathbb{R}}^n \\to {\\mathbb{R}}^n$, we\n    can extend it to an operator on\n    $T'\\colon {\\mathbb{R}}^{n+1} \\to {\\mathbb{R}}^{n+1}$ by having it\n    act as $T$ on the lower $n$ coordinates, and setting\n    $T'(e_{n+1}) = -e_{n+1}$. Since $\\det T' = -\\det T = 1$, our new\n    $T'$ will be a rotation (the extra angle of rotation will be by\n    $\\pi$) that reflects sets in the ${\\mathbb{R}}^n$ plane.\n\n[3]{} Friedberg, Insel, Spence. [*Linear Algebra*]{}. Prentice-Hall,\n1997. Vladimir I. Arnol\u2019d (trans. Roger Cooke). [*Ordinary Differential\nEquations*]{}. Springer-Verlag, 1992.\n\n[^1]: Because the complex roots occur in conjugate pairs, the\n    information about the sign of $\\theta$ is lost. This too, is not a\n    surprise, because the sign of $\\theta$, i.e. whether the rotation is\n    clockwise or counterclockwise, is dependent on the orientation of\n    the basis vectors.\n\n[^2]: In fact, the space $W$ constructed is two-dimensional if and only\n    if the eigenvalue $\\lambda$ is not purely real. Compare with the\n    remark (i) after the proof of Theorem 1.\n\n    Actually, Lemma 1 has more uses than just proving Theorem 1. For\n    example, if $\\dot{x} = Ax$ is a linear differential equation, and\n    the constant coefficient matrix $A$ has only simple eigenvalues,\n    then it is a consequence of Lemma 1, that the differential equation\n    decomposes into a series of disjoint one-variable and two-variable\n    equations. The solutions are then readily understood: they are\n    always of the form of an exponential multiplied by a sinusoid, and\n    linear combinations thereof. The sinusoids will be present whenever\n    there are non-real eigenvalues.",
  "citations": [
    {
      "textCitation": "https://planetmath.org/DecompositionOfOrthogonalOperatorsAsRotationsAndReflections"
    }
  ],
  "indexable": true,
  "names": [
    "decomposition of orthogonal operators as rotations and reflections"
  ]
}