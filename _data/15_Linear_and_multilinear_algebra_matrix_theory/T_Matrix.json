{
  "alternatePhrases": [],
  "definition": "A matrix is simply a mapping $M\\colon A\\times B\\to C$ of the product of\ntwo sets into some third set. As a rule, though, the word matrix and the\nnotation associated with it are used only in connection with linear\nmappings. In such cases $C$ is the ring or field of scalars.\n\n**Matrix of a linear mapping**\n\n**Definition:** Let $V$ and $W$ be finite-dimensional vector spaces over\nthe same field $k$, with bases $A$ and $B$ respectively, and let\n$f\\colon V\\to W$ be a linear mapping. For each $a\\in A$ let\n$(k_{ab})_{b\\in B}$ be the unique family of scalars (elements of $k$)\nsuch that $$f(a)=\\sum_{b\\in B}k_{ab}b\\;.$$ Then the family $(M_{ab})$\n(or equivalently the mapping $(a,b)\\mapsto M_{ab}$ of $A\\times B\\to k$)\nis called the matrix of $f$ with respect to the given bases $A$ and $B$.\nThe scalars $M_{ab}$ are called the *components* of the matrix. The\nmatrix $M$ is said to be of *size*\n${\\lvert A\\rvert}$-by-${\\lvert B\\rvert}$ or simply\n${\\lvert A\\rvert}$x${\\lvert B\\rvert}$ matrix.\n\nThe matrix describes the function $f$ completely; for any element\n$$x=\\sum_{a\\in A}x_aa$$ of $V$, we have $$f(x)=\\sum_{a\\in A}M_{ab}b$$ as\nis readily verified.\n\nAny two linear mappings $V\\to W$ have a sum, defined pointwise; it is\neasy to verify that the matrix of the sum is the sum, componentwise, of\nthe two given matrices.\n\nThe formalism of matrices extends somewhat to linear mappings between\n*modules*, i.e. extends to a ring $k$, not necessarily commutative,\nrather than just a field.\n\n**Rows and columns; product of two matrices**\n\nSuppose we are given three modules $V,W,X$, with bases $A,B,C$\nrespectively, and two linear mappings $f\\colon V\\to W$ and\n$g\\colon W\\to X$. $f$ and $g$ have some matrices $(M_{ab})$ and\n$(N_{bc})$ with respect to those bases. The *product* matrix $NM$ is\ndefined as the matrix $(P_{ac})$ of the function $$x\\mapsto g(f(x))$$\n$$V\\to W$$ with respect to the bases $A$ and $C$. Straight from the\ndefinitions of a linear mapping and a basis, one verifies that\n$$\\label{eq:prodcomp}\nP_{ac}=\\sum_{b\\in B}M_{ab}N_{bc}$$ for all $a\\in A$ and $c\\in C$.\n\nTo illustrate the notation of matrices in terms of rows and columns,\nsuppose the spaces $V,W,X$ have dimensions 2, 3, and 2 respectively, and\nbases $$A=\\{a_1,a_2\\}\\qquad B=\\{b_1,b_2,b_3\\}\\qquad C=\\{c_1,c_2\\}\\;.$$\nWe write $$\\begin{pmatrix}\nM_{11} & M_{12} & M_{13} \\\\\nM_{21} & M_{22} & M_{23}\n\\end{pmatrix}\n\\begin{pmatrix}\nN_{11} & N_{12} \\\\\nN_{21} & N_{22} \\\\\nN_{31} & N_{32}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nP_{11} & P_{12} \\\\\nP_{21} & P_{22}\n\\end{pmatrix}\\;.$$ (Notice that we have taken a liberty with the\nnotation, by writing e.g. $M_{12}$ instead of $M_{a_1a_2}$.) The\nequation shows that the multiplication of two matrices proceeds \u201crows by\ncolumns\u201d. Also, in an expression such as $N_{23}$, the first index\nrefers to the row, and the second to the column, in which that component\nappears.\n\nSimilar notation can describe the calculation of $f(x)$ whenever $f$ is\na linear mapping. For example, if $f\\colon V\\to W$ is linear, and\n$x=\\sum_ix_ia_i$ and $f(x)=\\sum_iy_ib_i$, we write $$\\begin{pmatrix}\nx_1 & x_2\n\\end{pmatrix}\n\\begin{pmatrix}\nM_{11} & M_{12} & M_{13} \\\\\nM_{21} & M_{22} & M_{23}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_1 & y_2 & y_3 \\\\\n\\end{pmatrix}\\;.$$ When, as above, a \u201crow vector\u201d denotes an element of\na space, a \u201ccolumn vector\u201d denotes an element of the dual space. If,\nsay, $\\overline{f}\\colon W^*\\to V^*$ is the transpose of $f$, then, with\nrespect to the bases dual to $A$ and $B$, an equation\n$\\overline{f}(\\sum_j\\nu_j\\beta_j)=\\sum_i\\mu_i\\alpha_i$ may be written\n$$\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}\n=\n\\begin{pmatrix}\nM_{11} & M_{12} & M_{13} \\\\\nM_{21} & M_{22} & M_{23}\n\\end{pmatrix}\n\\begin{pmatrix} \\nu_1 \\\\ \\nu_2 \\\\ \\nu_3 \\end{pmatrix}\\;,$$ One more\nillustration: Given a bilinear form $L\\colon V\\times W\\to k$, we can\ndenote $L(v,w)$ by $$\\begin{pmatrix} v_1 & v_2 \\end{pmatrix}\n\\begin{pmatrix}\nL_{11} & L_{12} & L_{13} \\\\\nL_{21} & L_{22} & L_{23}\n\\end{pmatrix}\n\\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix}\\;.$$\n\n**Square matrix**\n\nA matrix $M\\colon A\\times B\\to C$ is called square if $A=B$, or if some\nbijection $A\\to B$ is implicit in the context. (It is not enough for $A$\nand $B$ to be equipotent.) Square matrices naturally arise in connection\nwith a linear mapping of a space into *itself* (called an endomorphism),\nand in the related case of a change of basis (from one basis of some\nspace, to another basis of the same space). When $A$ is finite of\ncardinality $n$ (and thus, so is $B$), then $n$ is often called the\n*order* of the matrix $M$. Unfortunately, equally often order of $M$\nmeans the of $M$ as an element of the .\n\n**Miscelleous usages of \u201cmatrix\u201d**\n\nThe word matrix has come into use in some areas where linear mappings\nare not at issue. An example would be a combinatorical statement, such\nas Hall\u2019s marriage theorem, phrased in terms of \u201c0-1 matrices\u201d instead\nof subsets of $A\\times B$.\n\n**Remark**\n\nMatrices are heavily used in the physical sciences, engineering,\nstatistics, and computer programming. But for purely mathematical\npurposes, they are less important than one might expect, and indeed are\nfrequently irrelevant in linear algebra. Linear mappings, determinants,\ntraces, transposes, and a number of other simple notions can and should\nbe defined without matrices, simply because they have a meaning\nindependent of any basis or bases. Many little theorems in linear\nalgebra can be proved in a simpler and more enlightening way without\nmatrices than with them. One more illustration: The derivative (at a\npoint) of a mapping from one surface to another is a linear mapping; it\nis not a matrix of partial derivatives, because the matrix depends on a\nchoice of basis but the derivative does not.",
  "language": "INFORMAL",
  "phrase": "Matrix",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/Matrix"
    }
  ],
  "indexable": true
}