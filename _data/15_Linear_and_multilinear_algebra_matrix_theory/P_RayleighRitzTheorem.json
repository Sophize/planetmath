{
  "language": "INFORMAL",
  "remarks": "",
  "statement": "Let $A\\in \\mathbf{C}^{n\\times n}$ be a Hermitian matrix. Then its\neigenvectors are the critical points (vectors) of the \u201cRayleigh\nquotient\u201d, which is the real function\n$R:\\mathbb{C}^{n}\\backslash \\{\\mathbf{0}%\n\\}\\rightarrow \\mathbb{R}$$$R(\\mathbf{x})=\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}},\\Vert \\mathbf{x}\\Vert \\neq 0$$\n\nand its eigenvalues are its values at such critical points.\n\nAs a consequence, we have:\n$$\\lambda_{max}=\\max_{\\Vert \\mathbf{x}\\Vert \\neq 0}\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$$\nand\n$$\\lambda_{min}=\\min_{\\Vert \\mathbf{x}\\Vert \\neq 0}\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$$\n\nProof:\n\nFirst of all, let\u2019s observe that for a hermitian matrix, the number\n$\\mathbf{x}^{H}A\\mathbf{x}$ is a real one (actually,\n$<\\mathbf{x},A\\mathbf{x}>=$ $%\n\\mathbf{x}^{H}A\\mathbf{x=(}A^{H}\\mathbf{x)}^{H}\\mathbf{x=(}A\\mathbf{x)}^{H}%\n\\mathbf{x=}<A\\mathbf{x},\\mathbf{x>=}<\\mathbf{x},A\\mathbf{x>}^{\\ast }$,\nwhence $<\\mathbf{x},A\\mathbf{x>=x}^{H}A\\mathbf{x}$ is real), so that the\nRayleigh quotient is real as well.\n\nLet\u2019s now compute the critical points $\\overline{\\mathbf{x}}$ of the\nRayleigh quotient, i.e. let\u2019s solve the equations system\n$\\frac{dR(\\overline{%\n\\mathbf{x}})}{d\\mathbf{x}}=\\mathbf{0}^{T}$. Let\u2019s write\n$\\mathbf{x=x}^{(R)}+j%\n\\mathbf{x}^{(I)}$, $\\mathbf{x}^{(R)}$ and $\\mathbf{x}^{(I)}$ being\nrespectively the real and imaginary part of $\\mathbf{x}$. We have:\n\n$$\\frac{dR(\\mathbf{x})}{d\\mathbf{x}}=\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(R)}}+j\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(I)}}$$\n\nso that we must\nhave:$$\\frac{dR(\\overline{\\mathbf{x}})}{d\\mathbf{x}^{(R)}}=\\frac{dR(\\overline{\\mathbf{x}})}{d\\mathbf{x}^{(I)}}=\\mathbf{0}^{T}$$\n\nUsing derivatives rules, we obtain:\n\n$$\\begin{aligned}\n\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(R)}} &=&\\frac{d}{d\\mathbf{x}^{(R)}}\n\\left( \\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}\\right) =\n\\frac{\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(R)}}\n\\mathbf{x}^{H}\\mathbf{x-x}^{H}A\\mathbf{x}\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{\nx}\\right) }{d\\mathbf{x}^{(R)}}}{\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) ^{2}}=\\frac{\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(R)}}\n-R(\\mathbf{x})\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) }{d\\mathbf{x}\n^{(R)}}}{\\mathbf{x}^{H}\\mathbf{x}}\\text{.}\\end{aligned}$$\n\nApplying matrix calculus rules, we find: $$\\begin{aligned}\n\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(R)}} &=&%\n\\mathbf{x}^{H}A\\frac{d\\mathbf{x}}{d\\mathbf{x}^{(R)}}+\\mathbf{x}^{T}A^{T}%\n\\frac{d\\mathbf{x}^{\\ast }}{d\\mathbf{x}^{(R)}}=\\mathbf{x}^{H}A+\\mathbf{x}^{T}A^{T}=\\mathbf{x}^{H}A+\\left( \\mathbf{x}%\n^{H}A^{H}\\right) ^{\\ast }=\\end{aligned}$$\n\nand since\n$A=A^{H}$,$$=\\mathbf{x}^{H}A+\\left( \\mathbf{x}^{H}A\\right) ^{\\ast }=2\\Re\\left( \n\\mathbf{x}^{H}A\\right) \\text{.}$$\n\nIn a similar way, we\nget:$$\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) }{d\\mathbf{x}^{(R)}}=2\\Re%\n\\left( \\mathbf{x}^{H}\\right) \\text{.}$$\n\nSubstituting, we\nobtain:$$\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(R)}}=2\\frac{\\Re\\left( \\mathbf{x}^{H}A\\right) -R(\\mathbf{x})\\Re\\left( \\mathbf{x}^{H}\\right) }{\\mathbf{x}^{H}\\mathbf{x}}$$\n\nand, after a transposition, equating to the null column\nvector,$$\\begin{aligned}\n\\mathbf{0} &=&\\left( \\Re\\left( \\overline{\\mathbf{x}}^{H}A\\right) -R(%\n\\overline{\\mathbf{x}})\\Re\\left( \\overline{\\mathbf{x}}^{H}\\right)\n\\right) ^{T}= \\\\\n&=&\\Re\\left( A^{T}\\overline{\\mathbf{x}}^{\\ast }\\right) -R(\\overline{%\n\\mathbf{x}})\\Re\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) =\\Re%\n\\left( (A^{H}\\overline{\\mathbf{x}}\\mathbf{)}^{\\ast }\\right) -R(\\overline{%\n\\mathbf{x}})\\Re\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) = \\\\\n&=&\\Re\\left( (A\\overline{\\mathbf{x}}\\mathbf{)}^{\\ast }\\right) -R(%\n\\overline{\\mathbf{x}})\\Re\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) =%\n\\Re(A\\overline{\\mathbf{x}}\\mathbf{)}-R(\\overline{\\mathbf{x}})\\Re%\n\\left( \\overline{\\mathbf{x}}\\right)\\end{aligned}$$and, since\n$R(\\mathbf{x})$ is\nreal,$$\\Re(A\\overline{\\mathbf{x}}-R(\\overline{\\mathbf{x}})\\overline{\\mathbf{x}}\\mathbf{)=0}$$\n\nLet\u2019s then evaluate\n$\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(I)}}$:$$\\begin{aligned}\n\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(I)}} &=&\\frac{d}{d\\mathbf{x}^{(I)}}%\n\\left( \\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}\\right) =%\n\\frac{\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(I)}}%\n\\mathbf{x}^{H}\\mathbf{x-x}^{H}A\\mathbf{x}\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{%\nx}\\right) }{d\\mathbf{x}^{(I)}}}{\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) ^{2}}=\n\\frac{\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(I)}}%\n-R(\\mathbf{x})\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) }{d\\mathbf{x}%\n^{(I)}}}{\\mathbf{x}^{H}\\mathbf{x}}\\text{.}\\end{aligned}$$\n\n\u00a0Applying again matrix calculus rules, we find: $$\\begin{aligned}\n\\frac{d\\left( \\mathbf{x}^{H}A\\mathbf{x}\\right) }{d\\mathbf{x}^{(I)}} &=&%\n\\mathbf{x}^{H}A\\frac{d\\mathbf{x}}{d\\mathbf{x}^{(I)}}+\\mathbf{x}^{T}A^{T}\\frac{d\\mathbf{x}^{\\ast }}{d\\mathbf{x}^{(I)}}=j\\mathbf{x}^{H}A-j\\mathbf{x}^{T}A^{T}=j(\\mathbf{x}^{H}A-\\left( \\mathbf{x}^{H}A^{H}\\right) ^{\\ast })=\\end{aligned}$$\n\nand since $A=A^{H}$,\n$$=j(\\mathbf{x}^{H}A-\\left( \\mathbf{x}^{H}A\\right) ^{\\ast })=j(2j\\Im\\left( \\mathbf{x}^{H}A\\right) )=-2\\Im\\left( \\mathbf{x}^{H}A\\right) \n\\text{.}$$\n\nIn a similar way, we get:\n$$\\frac{d\\left( \\mathbf{x}^{H}\\mathbf{x}\\right) }{d\\mathbf{x}^{(I)}}=j\\mathbf{x}^{H}-j\\mathbf{x}^{T}=j(\\mathbf{x}^{H}-\\left( \\mathbf{x}^{H}\\right) ^{\\ast\n})=j(2j\\Im(\\mathbf{x}^{H}))=-2\\Im\\left( \\mathbf{x}^{H}\\right) \n\\text{.}$$\n\nSubstituting, we\nobtain:$$\\frac{dR(\\mathbf{x})}{d\\mathbf{x}^{(I)}}=-2\\frac{\\Im\\left( \\mathbf{x}^{H}A\\right) -R(\\mathbf{x})\\Im\\left( \\mathbf{x}^{H}\\right) }{\\mathbf{x}^{H}\\mathbf{x}}$$\n\nand, after a transposition, equating to the null column\nvector,$$\\begin{aligned}\n\\mathbf{0} &=&\\left( \\Im\\left( \\overline{\\mathbf{x}}^{H}A\\right) -R(%\n\\overline{\\mathbf{x}})\\Im\\left( \\overline{\\mathbf{x}}^{H}\\right)\n\\right) ^{T}= \\\\\n&=&\\Im\\left( A^{T}\\overline{\\mathbf{x}}^{\\ast }\\right) -R(\\overline{%\n\\mathbf{x}})\\Im\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) =\\Im%\n\\left( (A^{H}\\overline{\\mathbf{x}}\\mathbf{)}^{\\ast }\\right) -R(\\overline{%\n\\mathbf{x}})\\Im\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) = \\\\\n&=&\\Im\\left( (A\\overline{\\mathbf{x}}\\mathbf{)}^{\\ast }\\right) -R(%\n\\overline{\\mathbf{x}})\\Im\\left( \\overline{\\mathbf{x}}^{\\ast }\\right) =-%\n\\Im(A\\overline{\\mathbf{x}}\\mathbf{)}+R(\\overline{\\mathbf{x}})\\Im%\n\\left( \\overline{\\mathbf{x}}\\right)\\end{aligned}$$\n\nand, since $R(\\mathbf{x})$ is real,\n$$\\Im(A\\overline{\\mathbf{x}}-R(\\overline{\\mathbf{x}})\\overline{\\mathbf{x}\n}\\mathbf{)=0}$$\n\nIn conclusion, we have that a stationary vector $\\overline{\\mathbf{x}}$\nfor the Rayleigh quotient satisfies the complex eigenvalue\nequation$$A\\overline{\\mathbf{x}}-R(\\overline{\\mathbf{x}})\\overline{\\mathbf{x}}=\\mathbf{0}$$\n\nwhence the thesis.$\\square $\n\nRemarks:\n\n1\\) The two relations\n$\\lambda _{\\max }=\\max_{\\Vert \\mathbf{x}\\Vert\\neq 0}\n\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$ and $\\lambda\n_{\\min }=\\min_{\\Vert \\mathbf{x\\Vert \\neq 0}}\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{%\n\\mathbf{x}^{H}\\mathbf{x}}$ can also be obtained in a simpler way. By\nSchur\u2019s canonical form theorem, any normal (and hence any hermitian)\nmatrix is unitarily diagonalizable, i.e. a unitary matrix $U$ exists\nsuch that $A=U\\Lambda U^{H}$ with $\\Lambda =diag(\\lambda _{1},\\lambda\n_{2},\\cdots,\\lambda _{n})$. So, since all eigenvalues of a hermitian\nmatrix are real, it\u2019s possible to write: $$\\begin{aligned}\n\\mathbf{x}^{H}A\\mathbf{x} &\\mathbf{=x}&^{H}U\\Lambda U^{H}\\mathbf{x=}\\left(\nU^{H}\\mathbf{x}\\right) ^{H}\\Lambda \\left( U^{H}\\mathbf{x}\\right) =\\mathbf{y}^{H}\\Lambda \\mathbf{y}=\\sum_{i=1}^{n}\\lambda _{i}\\left\\vert y_{i}\\right\\vert\n^{2}\\leq \\lambda _{\\max }\\sum_{i=1}^{n}\\left\\vert y_{i}\\right\\vert ^{2}= \\\\\n&=&\\lambda _{\\max }\\mathbf{y}^{H}\\mathbf{y=}\\lambda _{\\max }\\left( U^{H}\\mathbf{x}\\right) ^{H}\\left( U^{H}\\mathbf{x}\\right) =\\lambda _{\\max }\\left( \n\\mathbf{x}^{H}UU^{H}\\mathbf{x}\\right) \\mathbf{=}\\lambda _{\\max }\\left( \n\\mathbf{x}^{H}\\mathbf{x}\\right) \\end{aligned}$$ whence\n$$\\lambda _{\\max }\\geq \\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$$\nBut, having defined $A\\mathbf{v}_{M}=\\lambda _{\\max }\\mathbf{v}_{M}$, we\nhave:\n$$\\frac{\\mathbf{v}_{M}^{H}A\\mathbf{v}_{M}}{\\mathbf{v}_{M}^{H}\\mathbf{v}_{M}}=\\frac{\\mathbf{v}_{M}^{H}\\lambda _{\\max }\\mathbf{v}_{M}}{\\mathbf{v}_{M}^{H}\\mathbf{v}_{M}}=\\lambda _{\\max }$$\nso that\n$$\\lambda _{\\max }=\\max_{\\Vert \\mathbf{x\\Vert \\neq 0}}\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$$\nIn a much similar way, we obtain\n$$\\lambda _{\\min }=\\min_{\\Vert \\mathbf{x\\Vert \\neq 0}}\\frac{\\mathbf{x}^{H}A\\mathbf{x}}{\\mathbf{x}^{H}\\mathbf{x}}$$\n\n2\\) The above relations yield the following noteworthing bounds for the\ndiagonal entries of a hermitian matrix:\n$$\\lambda _{\\min }\\leq a_{ii}\\leq \\lambda _{\\max }$$ In fact, having\ndefined $$\\mathbf{e}_{i}=[\\overbrace{0,0,...,0,}^{i-1}1,0,...,0]^{T}$$\nand observing that\n$a_{ii}=\\frac{\\mathbf{e}_{i}^{H}A\\mathbf{e}_{i}}{\\mathbf{e}_{i}^{H}\\mathbf{e}_{i}}=R(\\mathbf{e}_{i})$,\nwe have:\n$$\\lambda _{\\min }=\\min_{\\Vert \\mathbf{x\\Vert \\neq 0}}R(\\mathbf{x})\\leq R(\\mathbf{e}_{i})\\leq \\max_{\\Vert \\mathbf{x\\Vert \\neq 0}}R(\\mathbf{x})=\\lambda\n_{\\max }\\text{.}$$",
  "citations": [
    {
      "textCitation": "https://planetmath.org/RayleighRitzTheorem"
    }
  ],
  "indexable": true,
  "names": [
    "Rayleigh-Ritz theorem"
  ]
}