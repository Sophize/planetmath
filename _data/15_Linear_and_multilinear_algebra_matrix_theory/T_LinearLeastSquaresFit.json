{
  "alternatePhrases": [],
  "definition": "One of the most common uses of least squares fitting is fitting a\nstraight line to data. Whilst, in general, it is difficult to determine\nthe curve which best fits the data, in this case there is a relatively\nsimple formula which can be used.\n\nSuppose we have a data set $(x_1,y_1), \\ldots, (x_n,y_n)$. Then the\nstraight line which best fits this set is given as\n$$y = {ns - pq \\over nr - p^2} x + {qr - ps \\over nr - p^2}$$ where\n$$\\begin{aligned}\np &= \\sum_{k=1}^n x_k \\\\\nq &= \\sum_{k=1}^n y_k \\\\\nr &= \\sum_{k=1}^n x_k^2 \\\\\ns &= \\sum_{k=1}^n x_k y_k\\end{aligned}$$\n\nBeing the best fitting line means minimizing the merit function $M$,\ngiven as $$M(a,b) = \\sum_{k=0}^n (a x_k + b - y_k)^2$$ with respect to\nthe parameters $a$ and $b$. Expanding the square, this can be written as\n$$M(a,b) = r a^2 + 2pab + nb^2 - 2sa - 2qb + t$$ where $p,q,r,s$ are as\nabove and $$t = \\sum_{k=1}^n y_k^2 .$$\n\nThis function $M$ is a quadratic polynomial; moreover, from its\ndefinition as a sum of squares, it is clear that the highest order terms\nare positive definite, hence it has a minimum and all that remains is to\nfind that minimum. To do this, we set the derivatives equal to zero to\nobtain the following equations: $$\\begin{aligned}\n0 = {\\partial M(a,b) \\over \\partial a} & = 2ar + 2pb - 2s \\\\\n0 = {\\partial M(a,b) \\over \\partial b} & = 2pa + 2nb - 2q\\end{aligned}$$\nThese equations are easily solved to give $$\\begin{aligned}\na &= {ns - pq \\over nr - p^2} \\\\\nb &= {qr - ps \\over nr - p^2} ;\\end{aligned}$$ substituting in the\nequation $y = ax  + b$ for a straight line, we obtain the answer given\nabove.\n\nBecause of the ease with which one can make a least squares fit of a\nline, this technique is often adapted to fitting other sorts of curves\nby making a change of variables. Two common cases of this practice are\npower laws and exponentials.\n\nSuppose that one wants to fit some data to a curve of the form\n$y = c e^{kx}$. Making a change of variable $y = e^u$ and defining\n$b = \\log c$, the equation of the curve becomes $u = kx + b$. One can\ntherefore fit the data set $(x_1, \\log y_1),\n\\ldots (x_n, \\log y_n)$ to a straight line.\n\nSuppose that one wants to fit some data to a curve of the form\n$y = cx^p$. Making a change of variable $x = e^v$, $y = e^u$ and\ndefining $b = \\log c$, the equation of the curve becomes $u = pv + b$.\nOne can therefore fit the data set $(\\log x_1, \\log y_1),\n\\ldots (\\log x_n, \\log y_n)$ to a straight line.\n\nAlthough convenient and common, this procedure can be a cheat because\nchanging variables and making a least squares fit of a line is not the\nsame as making a least squares fit to a curve. The reason for this is\nthat the merit functions are different and will not, in general have a\nminimum in the same place. However, if the data happen to approximately\nlie on a power curve or an exponential, then the answer obtained by\nchanging variables and fitting will be an approximation to the correct\nanswer. Depending on what one is doing, this approximation may be good\nenough or one may use it as a starting point for some algorithm to\ncompute the correct minimum.",
  "language": "INFORMAL",
  "phrase": "Linear Least Squares Fit",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/LinearLeastSquaresFit"
    }
  ],
  "indexable": true
}