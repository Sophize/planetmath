{
  "alternatePhrases": [],
  "definition": "A *random vector* is a finite-dimensional formal vector of random\nvariables. The random vector can be written either as a column or row of\nrandom variables, depending on its context and use. So if\n$X_1,X_2,\\ldots,X_n$ are random variables, then\n$$\\textbf{X}=\\begin{pmatrix} X_1 \\\\ X_2 \\\\\n\\vdots \\\\ X_n \\end{pmatrix}={(X_1,X_2,\\ldots,X_n)^{\\operatorname{T}}}$$\nis a random (column) vector. Similarly, one defines a *random matrix* to\nbe a formal matrix whose entries are all random variables. The size of a\nrandom vector and the size of a random matrix are assumed to be finite\nfixed constants.\n\nThe *distribution of a random vector* $\\textbf{X}=(X_1,X_2,\\ldots,X_n)$\nis defined to be the joint distribution of its coordinates\n$X_1,\\ldots,X_n$:\n$$F_{\\textbf{X}}(\\textbf{x}):=F_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n).$$\nSimilarly, the *distribution of a random matrix* is the joint\ndistribution of its matrix components.\n\nLet $\\textbf{X}=(X_1,X_2,\\ldots,X_n)$ be a random vector. If\n$\\operatorname{E}[X_i]$ exists ($<\\infty$) for each $i$, then the\nexpectation of $\\textbf{X}$, called the *mean vector* and denoted by\n$\\mathbf{E}[\\textbf{X}]$, is defined to be:\n$$\\mathbf{E}[\\textbf{X}]:=(\\operatorname{E}[X_1],\\operatorname{E}[X_2],\\ldots, \\operatorname{E}[X_n]).$$\nClearly $\\mathbf{E}[\\textbf{X}]^T=\\mathbf{E}[\\textbf{X}^T]$. The\nexpectation of a random matrix is similarly defined. Note that the\ndefinitions of expectations can also be defined via measure theory.\nThen, using Fubini\u2019s Theorem, one can show that the two sets of\ndefinitions coincide.\n\nAgain, let $\\textbf{X}=(X_1,X_2,\\ldots,X_n)^T$ be a random vector. If\n$\\boldsymbol{\\mu}$=$\\mathbf{E}[\\textbf{X}]$ is defined and\n$\\operatorname{E}[X_iX_j]$ are defined for all $1\\leq i,j \\leq n$, then\nthe variance of $\\textbf{X}$, denoted by $\\textbf{Var}[\\textbf{X}]$, is\ndefined to be:\n$$\\textbf{Var}[\\textbf{X}]:= \\mathbf{E}\\big[(\\textbf{X}-\\boldsymbol{\\mu})(\\textbf{X}-\\boldsymbol{\\mu})^T\\big].$$\nIt is not hard to see that $\\textbf{Var}[\\textbf{X}]$ is an $n\\times\nn$ symmetric matrix and it is equal to the covariance matrix of the\n$X_i$\u2019s.\n\n**:**\n\n1.  If **X** is an $n$-dimensional random vector with **A** a\n    $m\\times n$ constant matrix and $\\boldsymbol{\\alpha}$ an\n    $m$-dimensional constant vector, then\n    $$\\mathbf{E}[\\mathbf{AX}+\\boldsymbol{\\alpha}]=\\mathbf{AE}[\\mathbf{X}]+\\boldsymbol{\\alpha}.$$\n\n2.  Same set up as above. Then\n    $$\\mathbf{Var}[\\mathbf{AX}+\\boldsymbol{\\alpha}]=\\mathbf{AVar}[\\mathbf{X}]\\mathbf{A}^T.$$\n    If the ${X_i}$\u2019s are *iid* (independent identically distributed),\n    with variance $\\boldsymbol{\\sigma}^2$, then\n    $$\\mathbf{Var}[\\mathbf{AX}+\\boldsymbol{\\alpha}]=\\boldsymbol{\\sigma}^2\\mathbf{AA}^T.$$\n\n3.  Let $\\mathbf{X}$ be an $n$-dimensional random vector with\n    $\\boldsymbol{\\mu}=\\mathbf{E[X]}$,\n    $\\boldsymbol{\\Sigma}=\\mathbf{Var[X]}$. $\\mathbf{A}$ is an $n\\times\n    n$ constant matrix. Then\n    $$\\mathbf{E}[\\mathbf{X}^T\\mathbf{AX}]=\\operatorname{tr}(\\mathbf{A}\\boldsymbol{\\Sigma})+\n    \\boldsymbol{\\mu}^T\\mathbf{A}\\boldsymbol{\\mu}.$$",
  "language": "INFORMAL",
  "phrase": "Random Vector",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/RandomVector"
    }
  ],
  "indexable": true
}