{
  "alternatePhrases": [
    "sufficient estimator",
    "minimally sufficient statistic",
    "minimal sufficient",
    "minimally sufficient"
  ],
  "definition": "Let $\\lbrace f_\\theta \\rbrace$ be a statistical model with parameter\n$\\theta$. Let $\\boldsymbol{X}=(X_1,\\ldots,X_n)$ be a random vector of\nrandom variables representing $n$ observations. A statistic\n$T=T(\\boldsymbol{X})$ of $\\boldsymbol{X}$ for the parameter $\\theta$ is\ncalled a *sufficient statistic*, or a *sufficient estimator*, if the\nconditional probability distribution of $\\boldsymbol{X}$ given\n$T(\\boldsymbol{X})=t$ is not a function of $\\theta$ (equivalently, does\nnot depend on $\\theta$).\n\nIn other words, all the information about the unknown parameter $\\theta$\nis captured in the sufficient statistic $T$. If, say, we are interested\nin finding out the percentage of defective light bulbs in a shipment of\nnew ones, it is enough, or *sufficient*, to count the number of\ndefective ones (sum of the $X_i$\u2019s), rather than worrying about which\nindividual light bulbs are the defective ones (the vector\n$(X_1,\\ldots,X_n)$). By taking the sum, a certain \u201creduction\u201d of data\nhas been achieved.\n\n**Examples**\n\n1.  Let $X_1,\\ldots,X_n$ be $n$ independent observations from a uniform\n    distribution on integers $1,\\ldots,\\theta$. Let\n    $T=\\max\\lbrace X_1,\\ldots,X_n \\rbrace$ be a statistic for $\\theta$.\n    Then the conditional probability distribution of\n    $\\boldsymbol{X}=(X_1,\\ldots,X_n)$ given $T=t$ is\n    $$P(\\boldsymbol{X}\\mid t)=\\frac{P(X_1=x_1,\\ldots,X_n=x_n,\\max\\lbrace X_n\n    \\rbrace=t)}{P(\\max\\lbrace X_n \\rbrace=t)}.$$ The numerator is $0$ if\n    $\\max\\lbrace x_n\\rbrace\\neq t$. So in this case,\n    $P(\\boldsymbol{X}\\mid t)=0$ and is not a function of $\\theta$.\n    Otherwise, the numerator is $\\theta^{-n}$ and $P(\\boldsymbol{X}\\mid\n    t)$ becomes $$\\frac{\\theta^{-n}}{P(\\max\\lbrace X_n \\rbrace=t)}=\n    (\\theta^nP(X_{(1)}\\leq \\cdots\\leq X_{(n)}=t))^{-1},$$ where\n    $X_{(i)}$\u2019s are the rearrangements of the $X_i$\u2019s in a\n    non-decreasing order from $i=1$ to $n$. For the denominator, we\n    first note that $$\\begin{aligned}\n    P(X_{(1)}\\leq \\cdots\\leq X_{(n)}=t) &=& P(X_{(1)}\\leq \\cdots\\leq X_{(n)}\\leq t)-P(X_{(1)}\\leq \\cdots\\leq X_{(n)}<t) \\\\ &=& P(X_{(1)}\\leq \\cdots\\leq X_{(n)}\\leq t)-P(X_{(1)}\\leq \\cdots\\leq X_{(n)}\\leq t-1).\\end{aligned}$$\n    From the above equation, we find that there are $t^n-(t-1)^n$ ways\n    to form non-decreasing finite sequences of $n$ positive integers\n    such that the maximum of the sequence is $t$. So\n    $$(\\theta^nP(X_{(1)}\\leq \\cdots\\leq X_{(n)}=t))^{-1}=\n    (\\theta^n(t^n-(t-1)^n)\\theta^{-n})^{-1}=(t^n-(t-1)^n)^{-1}$$ again\n    is not a function of $\\theta$. Therefore, $T=\\max\\lbrace X_i\\rbrace$\n    is a sufficient statistic for $\\theta$. Here, we see that a\n    reduction of data has been achieved by taking only the largest\n    member of set of observations, not the entire set.\n\n2.  If we set $T(X_1,\\ldots,X_n)=(X_1,\\ldots,X_n)$, then we see that $T$\n    is trivially a sufficient statistic for *any* parameter $\\theta$.\n    The conditional probability distribution of $(X_1,\\ldots,X_n)$ given\n    $T$ is 1. Even though this is a sufficient statistic by definition\n    (of course, the individual observations provide as much information\n    there is to know about $\\theta$ as possible), and there is no loss\n    of data in $T$ (which is simply a list of all observations), there\n    is really no reduction of data to speak of here.\n\n3.  The sample mean $$\\overline{X}=\\frac{X_1+\\cdots+X_n}{n}$$ of $n$\n    independent observations from a normal distribution\n    $N(\\mu,\\sigma^2)$ (both $\\mu$ and $\\sigma^2$ unknown) is a\n    sufficient statistic for $\\mu$. This is the result of the\n    factorization criterion. Similarly, one sees that any partition of\n    the sum of $n$ observations $X_i$ into $m$ subtotals is a sufficient\n    statistic for $\\mu$. For instance,\n    $$T(X_1,\\ldots,X_n)=(\\sum_{i=1}^{j}X_i,\\sum_{i=j+1}^{k}X_i,\\sum_{i=k+1}^{n}X_i)$$\n    is a sufficient statistic for $\\mu$.\n\n4.  Again, assume there are $n$ independent observations $X_i$ from a\n    normal distribution $N(\\mu,\\sigma^2)$ with unknown mean and\n    variance. The sample variance\n    $$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2$$ is *not* a\n    sufficient statistic for $\\sigma^2$. However, if $\\mu$ is a known\n    constant, then $$\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\mu)^2$$ is a\n    sufficient statistic for $\\sigma^2$.\n\nA sufficient statistic for a parameter $\\theta$ is called a *minimal\nsufficient statistic* if it can be expressed as a function of any\nsufficient statistic for $\\theta$.\n\n**Example**. In example $3$ above, both the sample mean $\\overline{X}$\nand the finite sum $S=X_1+\\cdots+X_n$ are minimal sufficient statistics\nfor the mean $\\mu$. Since, by the factorization criterion, any\nsufficient statistic $T$ for $\\mu$ is a vector whose coordinates form a\npartition of the finite sum, taking the sum of these coordinates is just\nthe finite sum $S$. So, we have just expressed $S$ as a function of $T$.\nTherefore, $S$ is minimal. Similarly, $\\overline{X}$ is minimal.\n\nTwo sufficient statistics $T_1,T_2$ for a parameter $\\theta$ are said to\nbe equivalent provided that there is a bijection $g$ such that\n$g\\circ T_1=T_2$. $\\overline{X}$ and $S$ from the above example are two\nequivalent sufficient statistics. Two minimal sufficient statistics for\nthe same parameter are equivalent.",
  "language": "INFORMAL",
  "phrase": "Sufficient Statistic",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/SufficientStatistic"
    }
  ],
  "indexable": true
}