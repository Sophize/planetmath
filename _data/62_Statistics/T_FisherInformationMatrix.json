{
  "alternatePhrases": [
    "information matrix"
  ],
  "definition": "Given a statistical model\n$\\lbrace f_\\textbf{X}(\\boldsymbol{x}\\mid\\boldsymbol{\\theta})\\rbrace$ of\na random vector $\\textbf{X}$, the *Fisher information matrix*, $I$, is\nthe variance of the score function $U$. So, $$I=\\operatorname{Var}[U].$$\nIf there is only one parameter involved, then $I$ is simply called the\n*Fisher information* or *information* of\n$f_\\textbf{X}(\\boldsymbol{x}\\mid\\theta)$.\n\n**Remarks**\n\n-   If $f_\\textbf{X}(\\boldsymbol{x}\\mid\\boldsymbol{\\theta})$ belongs to\n    the exponential family,\n    $I=\\operatorname{E}\\big[U^{\\operatorname{T}}U\\big]$. Furthermore,\n    with some regularity conditions imposed, we have\n    $$I=-\\operatorname{E}\\Big[\\frac{\\partial U}{\\partial\\boldsymbol{\\theta}}\\Big].$$\n\n-   As an example, the normal distribution, $N(\\mu,\\sigma^2)$, belongs\n    to the exponential family and its log-likelihood function\n    $\\ell(\\boldsymbol{\\theta}\\mid x)$ is\n    $$-\\frac{1}{2}\\operatorname{ln}(2\\pi\\sigma^2)-\\frac{(x-\\mu)^2}{2\\sigma^2},$$\n    where $\\boldsymbol{\\theta}=(\\mu,\\sigma^2)$. Then the score function\n    $U(\\boldsymbol{\\theta})$ is given by\n    $$\\Big({\\frac{\\partial \\ell}{\\partial \\mu}},{\\frac{\\partial \\ell}{\\partial \\sigma^2}}\\Big) = \\Big(\\frac{x-\\mu}{\\sigma^2},\\frac{(x-\\mu)^2}{2\\sigma^4}-\\frac{1}{2\\sigma^2}\\Big).$$\n    Taking the derivative with respect to $\\boldsymbol{\\theta}$, we have\n    $$\\frac{\\partial U}{\\partial\\boldsymbol{\\theta}}=\n    \\begin{pmatrix}\n    \\displaystyle{{\\frac{\\partial U_1}{\\partial \\mu}}} & \\displaystyle{{\\frac{\\partial U_2}{\\partial \\mu}}} \\\\ \\ \\\\\n    \\displaystyle{{\\frac{\\partial U_1}{\\partial \\sigma^2}}} & \\displaystyle{{\\frac{\\partial U_2}{\\partial \\sigma^2}}} \\\\\n    \\end{pmatrix}=\n    \\begin{pmatrix}\n    \\displaystyle{\\frac{-1}{\\sigma^2}} & \\displaystyle{-\\frac{x-\\mu}{\\sigma^4}} \\\\ \\ \\\\\n    \\displaystyle{-\\frac{x-\\mu}{\\sigma^4}} & \\displaystyle{\\frac{1}{2\\sigma^4}-\\frac{(x-\\mu)^2}{\\sigma^6}}\n    \\end{pmatrix}.$$ Therefore, the Fisher information matrix $I$ is\n    $$-\\operatorname{E}\\Big[\\frac{\\partial U}{\\partial\\boldsymbol{\\theta}}\\Big]=\\frac{1}{2\\sigma^4}\n    \\begin{pmatrix} \n    2\\sigma^2 & 0 \\\\\n    0 & -1 \n    \\end{pmatrix}.$$\n\n-   Now, in linear regression model with constant variance $\\sigma^2$,\n    it can be shown that the Fisher information matrix $I$ is\n    $$\\frac{1}{\\sigma^2}\\textbf{X}^{\\operatorname{T}}\\textbf{X},$$ where\n    $\\textbf{X}$ is the design matrix of the regression model.\n\n-   In general, the Fisher information meansures how much \u201cinformation\u201d\n    is known about a parameter $\\theta$. If $T$ is an unbiased estimator\n    of $\\theta$, it can be shown that\n    $$\\operatorname{Var}\\big[T(X)\\big]\\ge\\frac{1}{I(\\theta)}$$ This is\n    known as the *Cramer-Rao inequality*, and the number $1/I(\\theta)$\n    is known as the *Cramer-Rao lower bound*. The smaller the variance\n    of the estimate of $\\theta$, the more information we have on\n    $\\theta$. If there is more than one parameter, the above can be\n    generalized by saying that\n    $$\\operatorname{Var}\\big[T(X)\\big]-I(\\boldsymbol{\\theta})^{-1}$$ is\n    positive semidefinite, where $I$ is the Fisher information matrix.",
  "language": "INFORMAL",
  "phrase": "Fisher Information Matrix",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/FisherInformationMatrix"
    }
  ],
  "indexable": true
}