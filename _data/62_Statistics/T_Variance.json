{
  "alternatePhrases": [],
  "definition": "Definition {#definition .unnumbered}\n----------\n\nThe *variance* of a real-valued random variable $X$ is\n$$\\operatorname{Var}X = {\\mathbb{E}}\\bigl[ (X - m)^2 \\bigr]\\,, \\quad m = {\\mathbb{E}}X\\,,$$\nprovided that both expectations ${\\mathbb{E}}X$ and\n${\\mathbb{E}}[(X-m)^2]$ exist.\n\nThe variance of $X$ is often denoted by $\\sigma^2(X)$, $\\sigma^2_X$, or\nsimply $\\sigma^2$. The exponent on $\\sigma$ is put there so that the\nnumber $\\sigma = \\sqrt{\\sigma^2}$ is measured in the same units as the\nrandom variable $X$ itself.\n\nThe quantity $\\sigma = \\sqrt{\\operatorname{Var}X}$ is called the\n*standard deviation* of $X$; because of the compatibility of the\nmeasuring units, standard deviation is usually the quantity that is\nquoted to describe an emprical probability distribution, rather than the\nvariance.\n\nUsage {#usage .unnumbered}\n-----\n\nThe variance is a measure of the dispersion or variation of a random\nvariable about its mean $m$.\n\nIt is not always the best measure of dispersion for all random\nvariables, but compared to other measures, such as the absolute mean\ndeviation, ${\\mathbb{E}}[ {\\lvertX-m\\rvert} ]$, the variance is the most\ntractable analytically.\n\nThe variance is closely related to the ${\\mathbf{L}}^2$ norm for random\nvariables over a probability space.\n\nProperties {#properties .unnumbered}\n----------\n\n1.  The variance of $X$ is the second moment of $X$ minus the square of\n    the first moment:\n    $$\\operatorname{Var}X  = {\\mathbb{E}}[X^2] - {\\mathbb{E}}[X]^2\\,.$$\n    This formula is often used to calculate variance analytically.\n\n2.  Variance is not a linear function. It scales quadratically, and is\n    not affected by shifts in the mean of the distribution:\n    $$\\operatorname{Var}[ aX + b ] = a^2 \\operatorname{Var}X\\,, \\quad \\text{ for any $a, b \\in {\\mathbb{R}}$.}$$\n\n3.  A random variable $X$ is constant almost surely if and only if\n    $\\operatorname{Var}X = 0$.\n\n4.  The variance can also be characterized as the minimum of expected\n    squared deviation of a random variable from any point:\n    $$\\operatorname{Var}X = \\inf_{a \\in {\\mathbb{R}}} {\\mathbb{E}}[(X-a)^2]\\,.$$\n\n5.  For any two random variables $X$ and $Y$ whose variances exist, the\n    variance of the linear combination $aX + bY$ can be expressed in\n    terms of their covariance:\n    $$\\operatorname{Var}[aX+bY] = a^2 \\operatorname{Var}X  + b^2 \\operatorname{Var}Y  + 2ab \\operatorname{Cov}[X,Y]\\,,$$\n    where\n    $\\operatorname{Cov}[X,Y] = {\\mathbb{E}}[(X-{\\mathbb{E}}X)(Y-{\\mathbb{E}}Y)]$,\n    and $a, b \\in {\\mathbb{R}}$.\n\n6.  For a random variable $X$, with actual observations\n    $x_1, \\dotsc, x_n$, its variance is often estimated empirically with\n    the *sample variance*:\n    $$\\operatorname{Var}X  \\approx s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\,,\n    \\quad\n    \\bar{x} = \\frac{1}{n} \\sum_{j=1}^n x_j\\,.$$",
  "language": "INFORMAL",
  "phrase": "Variance",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/Variance"
    }
  ],
  "indexable": true
}