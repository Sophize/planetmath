{
  "language": "INFORMAL",
  "remarks": "",
  "statement": "Let $X$ be a random variable, taking values in ${\\mathbb{R}}^n$,\nnormally distributed with a non-singular covariance matrix $\\Sigma$ and\na mean of zero.\n\nSuppose $Y$ is defined by $Y = B^* X $ for some linear transformation\n$B \\colon {\\mathbb{R}}^k \\to {\\mathbb{R}}^n$ of maximum rank. (${}^\\ast$\nto denotes the transpose operator.)\n\nThen the distribution of $X$ conditioned on $Y$ is multi-variate normal,\nwith conditional means and covariances of: $${\\mathbb{E}}[ X \\mid Y] = \n\\Sigma B {{({B^{\\ast}} \\Sigma B)}^{-1}} Y\n\\,, \\quad \\operatorname{Var}[X \\mid Y] = \\Sigma - \\Sigma B {{({B^{\\ast}} \\Sigma B)}^{-1}}  {(\\Sigma B)^{\\ast}}\\,.$$\n\nIf $k = 1$, so that $B$ is simply a vector in ${\\mathbb{R}}^n$, these\nformulas reduce to: $${\\mathbb{E}}[ X \\mid Y ]\n= \\frac{\\Sigma B Y}{\\operatorname{Var}[Y]}\\,, \\quad\n\\operatorname{Var}[X \\mid Y] = \\Sigma - \\frac{\\Sigma B {B^{\\ast}} \\Sigma}{\\operatorname{Var}[Y]} \\,.$$\n\nIf $X$ does not have zero mean, then the formula for\n${\\mathbb{E}}[X \\mid Y]$ is modified by adding ${\\mathbb{E}}[X]$ and\nreplacing $Y$ by $Y - {\\mathbb{E}}[Y]$, and the formula for\n$\\operatorname{Var}[X \\mid Y]$ is unchanged.\n\nWe split up $X$ into two stochastically independent parts, the first\npart containing exactly the information embodied in $Y$. Then the\nconditional distribution of $X$ given $Y$ is simply the unconditional\ndistribution of the second part that is independent of $Y$.\n\nTo this end, we first change variables to express everything in terms of\na *standard* multi-variate normal $Z$. Let\n$A \\colon {\\mathbb{R}}^n \\to {\\mathbb{R}}^n$ be a \u201csquare root\u201d\nfactorization of the covariance matrix $\\Sigma$, so that:\n$$A {A^{\\ast}} = \\Sigma\\,, \\quad Z = {{A}^{-1}} X\\,, \\quad X = AZ \\,, \\quad Y = {B^{\\ast}}AZ\\,.$$\n\nWe let $H \\colon {\\mathbb{R}}^n \\to {\\mathbb{R}}^n $ be the orthogonal\nprojection onto the range of\n$ {A^{\\ast}} B : {\\mathbb{R}}^k \\to {\\mathbb{R}}^n$, and decompose $Z$\ninto orthogonal components: $$Z = HZ + (I-H)Z \\,.$$ It is intuitively\nobvious that orthogonality of the two random normal vectors implies\ntheir stochastic independence. To show this formally, observe that the\nGaussian density function for $Z$ factors into a product:\n$$(2\\pi)^{-n/2} \\, \\exp\\bigl( -\\tfrac12 {\\lVertz\\rVert}^2 \\bigr)\n= (2\\pi)^{-n/2} \\, \\exp\\bigl( -\\tfrac12 {\\lVertHz\\rVert}^2 \\bigr) \\,\n                   \\exp\\bigl( -\\tfrac12 {\\lVert(I-H)z\\rVert}^2 \\bigr) \\,.$$\nWe can construct an orthonormal system of coordinates on\n${\\mathbb{R}}^n$ under which the components for $Hz$ are completely\ndisjoint from those components of $(I-H)z$. On the other hand, the\ndensities for $Z$, $HZ$, and $(I-H)Z$ remain invariant even after\nchanging coordinates, because they are radially symmetric. Hence the\nvariables $HZ$ and $(I-H)Z$ are separable in their joint density and\nthey are independent.\n\n$HZ$ embodies the information in the linear combination\n$Y = {B^{\\ast}}AZ$. For we have the identity:\n$$Y = \\bigl( {B^{\\ast}}A \\bigr) Z = \\bigl( {B^{\\ast}}A \\bigr) \\bigl( HZ + (I-H)Z \\bigr) \n= \\bigl( {B^{\\ast}}A \\bigr) HZ + 0\\,.$$ The last term is null because\n$(I-H)Z$ is orthogonal to the range of ${A^{\\ast}}B$ by definition.\n(Equivalently, $(I-H)Z$ lies in the kernel of\n${({A^{\\ast}} B)^{\\ast}} = {B^{\\ast}} A$.) Thus $Y$ can always be\nrecovered by a linear transformation on $HZ$.\n\nConversely, $Y$ completely determines $HZ$, from the analytical\nexpression for $H$ that we now give. In general, the orthogonal\nprojection onto the range of an injective transformation $T$ is\n$T {{({T^{\\ast}} T)}^{-1}} {T^{\\ast}}$. Applying this to\n$T = {A^{\\ast}}B$, we have $$\\begin{aligned}\nH &= {A^{\\ast}} B {{\\bigl( {B^{\\ast}} A {A^{\\ast}} B \\bigr)}^{-1}} {B^{\\ast}} A \\\\\n&= {A^{\\ast}} B {{({B^{\\ast}} \\Sigma B)}^{-1}} {B^{\\ast}} A \\,.\\end{aligned}$$\nWe see that $HZ = {A^{\\ast}} B {{ ({B^{\\ast}} \\Sigma B) }^{-1}} Y$.\n\nWe have proved that conditioning on $Y$ and $HZ$ are equivalent, and so:\n$${\\mathbb{E}}[Z \\mid Y] = {\\mathbb{E}}[Z \\mid HZ] = {\\mathbb{E}}[HZ + (I-H)Z \\mid HZ] = HZ + 0\\,,$$\nand $$\\begin{aligned}\n\\operatorname{Var}[Z \\mid Y] = \\operatorname{Var}[Z \\mid HZ] &= \\operatorname{Var}[HZ + (I-H)Z \\mid HZ] \\\\\n&= 0 + \\operatorname{Var}[(I-H)Z ] \\\\\n&= {\\mathbb{E}}\\bigl[ (I-H)Z {Z^{\\ast}} {(I-H)^{\\ast}} \\bigr] \\\\\n&= (I-H) {(I-H)^{\\ast}} \\\\\n&= I-H -{H^{\\ast}} + H{H^{\\ast}} = I-H\\,,\\end{aligned}$$ using the\ndefining property $H^2 = H = {H^{\\ast}}$ of orthogonal projections.\n\nNow we express the result in terms of $X$, and remove the dependence on\nthe transformation $A$ (which is not uniquely defined from the\ncovariance matrix):\n$${\\mathbb{E}}[X \\mid Y] = A \\, {\\mathbb{E}}[Z \\mid Y] = AHZ = \n\\Sigma B {{({B^{\\ast}} \\Sigma B)}^{-1}} Y$$ and\n$$\\operatorname{Var}[X \\mid Y] = A \\, \\operatorname{Var}[Z \\mid Y] \\, {A^{\\ast}}\n= A {A^{\\ast}} - A H {A^{\\ast}} \n= \\Sigma - \\Sigma B {{({B^{\\ast}} \\Sigma B)}^{-1}}  {B^{\\ast}} \\Sigma\\,.$$\n\nOf course, the conditional distribution of $X$ given $Y$ is the same as\nthat of $(I-H)Z$, which is multi-variate normal.\n\nThe formula in the statement of this theorem, for the single-dimensional\ncase, follows from substituting in\n$\\operatorname{Var}[Y] = \\operatorname{Var}[{B^{\\ast}}X] = {B^{\\ast}} \\Sigma B$.\nThe formula for when $X$ does not have zero mean follows from applying\nthe base case to the shifted variable $X - {\\mathbb{E}}[X]$.",
  "citations": [
    {
      "textCitation": "https://planetmath.org/ConditionalDistributionOfMultivariateNormalVariable"
    }
  ],
  "indexable": true,
  "names": [
    "conditional distribution of multi-variate normal variable"
  ]
}