{
  "alternatePhrases": [
    "$\\chi^2$ statistic",
    "chi-square statistic",
    "Pearson-chi-squared statistic",
    "Pearson-chi-square statistic"
  ],
  "definition": "Let $X$ be a discrete random variable with $m$ possible outcomes\n$x_1,\\ldots,x_m$ with probability of each outcome\n$\\operatorname{P}(X=x_i)=p_i$.\\\n\\\n$n$ independent observations are obtained where each observation has the\nsame distribution as $X$. Bin the observations into $m$ groups, so that\neach group contains all observations having the same outcome $x_i$.\nNext, count the number of observations in each group to get\n$n_1,\\ldots,n_k$ corresponding to the outcomes $x_1,\\ldots,x_k$, so that\n$n=\\sum n_i$. It is desired to find out how close the actual number of\noutcomes $n_i$ are to their expected values $np_i$.\\\n\\\nIntuitively, this \u201ccloseness\u201d depends on how big the sample is, and how\nlarge the deviations are between the observed and the expected, for all\ncategories. The value $$\\begin{aligned}\n\\chi^2=\\sum_{i=1}^{m} \\frac{(n_i-np_i)^2}{np_i},\\end{aligned}$$ called\nthe $\\chi^2$ *statistic*, or the *chi-squared statistic*, is such a\nmeasure of \u201ccloseness\u201d. It is also known as the *Pearson-chi-squared*\nstatistic, in honor of the English statistician Karl Pearson, who showed\nthat (1) has approximately a with $m-1$ degrees of freedom. The degree\nof freedom depends on the number of free variables in $\\chi^2$, and is\nnot always $m-1$, as we will see in Example $3$.\\\n\\\nUsually, $\\chi^2$ statistic is utilized in hypothesis testing, where the\nnull hypothesis specifies that the actual equals the expected. A large\nvalue of $\\chi^2$ means either the deviations from the expectations are\nlarge or the sample is small, and therefore, either the null hypothesis\nshould be rejected or there is not enough information to give a\nmeaningful interpretation. How large of a deviation, compared to the\nsample size, is enough to reject the null hypothesis depends on the\ndegree of freedom of chi-squared distribution of $\\chi^2$ and the\nspecified critical values.\\\n\\\n**Examples**.\n\n1.  Suppose a coin is tossed 10 times and 7 heads are observed. We would\n    like to know if the coin is fair based on the observations. We have\n    the following hypothesis:\n    $$H_0: p=\\frac{1}{2}\\qquad H_1:p\\neq\\frac{1}{2}.$$ Break up the\n    observations into two groups: heads and tails. Then, according to\n    $H_0$, $$\\chi^2=\\frac{(7-5)^2}{5}+\\frac{(3-5)^2}{5}=1.60.$$ Checking\n    the table of critical values of chi-squared distributions, we see\n    that at degree of freedom $=1$, there is a 0.100 chance that the\n    $\\chi^2$ value is higher than 2.706. Since $1.600<2.706$, we may not\n    want to reject the null hypothesis. However, we may not want to\n    outrightly accept it either simply because the sample size is not\n    very large.\n\n2.  Now, a coin is tossed 100 times and 70 heads are observed. Using the\n    same null hypothesis as above,\n    $$\\chi^2=\\frac{(70-50)^2}{50}+\\frac{(30-50)^2}{50}=16.00.$$ Even at\n    p-value $=0.005$, the corresponding critical value of 7.879 is quite\n    a bit smaller than 16. So we will reject the null hypothesis even at\n    confidence level 99.5%($=1-$p-value).\n\n3.  $\\chi^2$ statistic can be used in non-parametric situations as well,\n    particularly, in contingency tables. Three dice of varying sizes are\n    each tossed 100 times and the top faces are recorded. The results of\n    the count of each possible value of the top face, for each die is\n    summarized in the following table:\n\n       Die$\\backslash$top face   1    2    3    4    5    6    all\n      ------------------------- ---- ---- ---- ---- ---- ---- -----\n                Die 1            16   19   17   15   19   14   100\n                Die 2            17   18   14   13   22   16   100\n                Die 3            12   20   19   18   20   11   100\n              All dice           45   57   50   46   61   41   300\n\n    Let $X_i=$ count of top face$=i$, and $Y_j=$ Die $j$. Next, we want\n    to test the following hypotheses:\n    $$H_0: X_i\\mbox{ is independent of } Y_j\\qquad\n     H_1:\\mbox{otherwise}.$$ Since we do not know the exact distribution\n    of the top faces, we approximate the distribution by using the last\n    row. For example, the (marginal) probability that top face = 1 is\n    $\\frac{45}{300}=0.15$. This says that the probability that top face\n    = 1 in Die $i$ = $0.15\\times\\frac{1}{3}=0.05$. Then, based on the\n    null hypothesis, we have the following table of \u201cexpected count\u201d:\n\n       Die$\\backslash$top face    1      2      3      4      5      6\n      ------------------------- ------ ------ ------ ------ ------ ------\n                Die 1            15.0   19.0   16.7   15.3   20.3   13.7\n                Die 2            15.0   19.0   16.7   15.3   20.3   13.7\n                Die 3            15.0   19.0   16.7   15.3   20.3   13.7\n\n    For each die, we can compute the $\\chi^2$. For instance, for the\n    first die, $$\\begin{aligned}\n    \\chi^2&=&\\frac{(16-15.0)^2}{15.0}+\\frac{(19-19.0)^2}{19.0}+\n    \\frac{(17-16.7)^2}{16.7}+\\\\\n    &&\\frac{(15-15.3)^2}{15.3}+\n    \\frac{(19-20.3)^2}{20.3}+\\frac{(14-13.7)^2}{13.7}\\\\&=&0.176\\end{aligned}$$\n    The results are summarized in the following\n\n                  $\\chi^2$   degrees of freedom\n      ---------- ---------- --------------------\n        Die 1      0.176             5\n        Die 2      1.636             5\n        Die 3      1.969             0\n       All dice    3.781             10\n\n    Note that the degree of freedom for the last dice is 0 because the\n    expected counts in the last row are completely determined by those\n    in the first two rows (and the totals). Looking up the table, we see\n    that there is a $90\\%$ that the value of $\\chi^2$ will be greater\n    than $4.865$, and since $3.781<4.865$, we accept the null\n    hypothesis: the outcomes of the tosses have no bearing on which die\n    is tossed.\n\n**Remark.** In general, for a $p\\times q$ 2-way contingency table, the\n$\\chi^2$ statistic is given by $$\\begin{aligned}\n\\chi^2=\\sum_{i=1}^{p}\\sum_{j=1}^{q}\\frac{(n_{ij}-m_{ij})^2}{m_{ij}},\\end{aligned}$$\nwhere $n_{ij}$ and $m_{ij}$ are the actual and expected counts in Cell\n$(i,j)$. When the sample is large, $\\chi^2$ has a chi-squared\ndistribution with $(p-1)(q-1)$ degrees of freedom. In particular, when\ntesting for the independence between two categorical variables, the\nexpected count $m_{ij}$ is\n$$m_{ij}=\\frac{n_{i*}n_{*j}}{n},\\mbox{ where }\nn_{i*}=\\sum_{j=1}^{q}n_{ij},\\mbox{\n}n_{*j}=\\sum_{i=1}^{p}n_{ij},\\mbox{ and\n}n=\\sum_{i=1}^{p}\\sum_{j=1}^{q}n_{ij}.$$",
  "language": "INFORMAL",
  "phrase": "Chi-Squared Statistic",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/ChisquaredStatistic"
    }
  ],
  "indexable": true
}