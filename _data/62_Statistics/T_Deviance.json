{
  "alternatePhrases": [],
  "definition": "**Background**\n\nIn testing the fit of a generalized linear model $\\mathcal{P}$ of some\ndata (with response variable **Y** and explanatory variable(s) **X**),\none way is to compare $\\mathcal{P}$ with a similar model\n$\\mathcal{P}_0$. By similarity we mean: given $\\mathcal{P}$ with the\nresponse variable $Y_i\\sim f_{Y_i}$ and link function $g$ such that\n$g(\\operatorname{E}[Y_i])={\\textbf{X}_i}^{\\operatorname{T}}\\boldsymbol{\\beta}$,\nthe model $\\mathcal{P}_0$\n\n1.  is a generalized linear model of the same data,\n\n2.  has the response variable $Y$ distributed as $f_Y$, same as found in\n    $\\mathcal{P}$\n\n3.  has the same link function $g$ as found in $\\mathcal{P}$, such that\n    $g(\\operatorname{E}[Y_i])={\\textbf{X}_i}^{\\operatorname{T}}\\boldsymbol{\\beta_0}$\n\nNotice that the only possible difference is found in the parameters\n$\\boldsymbol{\\beta}$.\n\nIt is desirable for this $\\mathcal{P}_0$ to be served as a base model in\ncase when more than one models are being assessed. Two possible\ncandidates for $\\mathcal{P}_0$ are the *null model* and the *saturated\nmodel*. The null model $\\mathcal{P}_{null}$ is one in which only one\nparameter $\\mu$ is used so that $g(\\operatorname{E}[Y_i])=\\mu$, all\nresponses have the same predicted outcome. The saturated model\n$\\mathcal{P}_{max}$ is the other extreme where the maximum number of\nparameters are used in the model so that the observed response values\nequal to the predicted response values exactly,\n$g(\\operatorname{E}[Y_i])={\\textbf{X}_i}^{\\operatorname{T}}\\boldsymbol{\\beta}_{max}=y_i$\n\n**Definition** The *deviance* of a model $\\mathcal{P}$ (generalized\nlinear model) is given by\n$$\\operatorname{dev}(\\mathcal{P})=2\\big[\\ell(\\hat{\\boldsymbol{\\beta}}_{max}\\mid\\textbf{y})-\\ell(\\hat{\\boldsymbol{\\beta}}\\mid\\textbf{y})\\big],$$\nwhere $\\ell$ is the log-likelihood function, $\\hat{\\boldsymbol{\\beta}}$\nis the MLE of the parameter vector $\\boldsymbol{\\beta}$ from\n$\\mathcal{P}$ and $\\hat{\\boldsymbol{\\beta}}_{max}$ is the MLE of\nparameter vector $\\boldsymbol{\\beta}_{max}$ from the saturated model\n$\\mathcal{P}_{max}$.\n\n**Example** For a normal or general linear model, where the link\nfunction is the identity:\n$$\\operatorname{E}[Y_i]={\\textbf{x}_i}^{\\operatorname{T}}\\boldsymbol{\\beta},$$\nwhere the $Y_i$\u2019s are mutually independent and normally distributed as\n$N(\\mu_i,\\sigma^2)$. The log-likelihood function is given by\n$$\\ell(\\boldsymbol{\\beta}\\mid\\textbf{y})=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i-\\mu_i)^2-\\frac{n\\operatorname{ln}(2\\pi\\sigma^2)}{2},$$\nwhere $\\mu_i={\\textbf{x}_i}^{\\operatorname{T}}\\boldsymbol{\\beta}$ is the\npredicted response values, and $n$ is the number of observations.\n\nFor the model in question, suppose\n$\\hat{\\mu}_i={\\textbf{X}_i}^{\\operatorname{T}}\\hat{\\boldsymbol{\\beta}}$\nis the expected mean calculated from the maximum likelihood estimate\n$\\hat{\\boldsymbol{\\beta}}$ of the parameter vector $\\boldsymbol{\\beta}$.\nSo,\n$$\\ell(\\hat{\\boldsymbol{\\beta}}\\mid\\textbf{y})=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i-\\hat{\\mu}_i)^2-\\frac{n\\operatorname{ln}(2\\pi\\sigma^2)}{2},$$\n\nFor the saturated model $\\mathcal{P}_{max}$, the predicted value\n$(\\hat{\\mu}_{max})_i$ = the observed response value $y_i$. Therefore,\n$$\\ell(\\hat{\\boldsymbol{\\beta}}_{max}\\mid\\textbf{y})=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(y_i-(\\hat{\\mu}_{max})_i)^2-\\frac{n\\operatorname{ln}(2\\pi\\sigma^2)}{2}=-\\frac{n\\operatorname{ln}(2\\pi\\sigma^2)}{2}.$$\nSo the deviance is\n$$\\operatorname{dev}(\\mathcal{P})=2\\big[\\ell(\\hat{\\boldsymbol{\\beta}}_{max}\\mid\\textbf{y})-\\ell(\\hat{\\boldsymbol{\\beta}}\\mid\\textbf{y})\\big]=\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}(y_i-\\hat{\\mu}_i)^2,$$\nwhich is exactly the residual sum of squares, or RSS, used in regression\nmodels.\n\n**Remarks**\n\n-   The deviance is necessarily non-negative.\n\n-   The distribution of the deviance is asymptotically a with $n-p$\n    degress of freedom, where $n$ is the number of observations and $p$\n    is the number of parameters in the model $\\mathcal{P}$.\n\n-   If two generalized linear models $\\mathcal{P}_1$ and $\\mathcal{P}_2$\n    are nested, say $\\mathcal{P}_1$ is nested within $\\mathcal{P}_2$, we\n    can perform hypothesis testing $H_0$: the model for the data is\n    $\\mathcal{P}_1$ with $p_1$ parameters, against $H_1$: the model for\n    the data is the more general $\\mathcal{P}_2$ with $p_2$ parameters,\n    where $p_1<p_2$. The deviance difference\n    $\\Delta$(dev)$=\\operatorname{dev}(\\mathcal{P}_2)-\\operatorname{dev}(\\mathcal{P}_1)$\n    can be used as a test statistic and it is approximately a chi square\n    distribution with $p_2-p_1$ degrees of freedom.\n\n[8]{} P. McCullagh and J. A. Nelder, [*Generalized Linear Models*]{},\nChapman & Hall/CRC, 2nd ed., London (1989). A. J. Dobson, [*An\nIntroduction to Generalized Linear Models*]{}, Chapman & Hall, 2nd ed.\n(2001).",
  "language": "INFORMAL",
  "phrase": "Deviance",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/Deviance"
    }
  ],
  "indexable": true
}