{
  "alternatePhrases": [],
  "definition": "Given a binary respose variable $Y$ with probability of success $p$, the\n*logistic regression* is a non-linear regression model with the\nfollowing model equation:\n$$\\operatorname{E}[Y]=\\frac{\\operatorname{exp}(\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta})}\n{1+\\operatorname{exp}(\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta})},$$\nwhere $\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta}$ is the\nproduct of the transpose of the column matrix $\\boldsymbol{X}$ of\nexplanatory variables and the unknown column matrix $\\boldsymbol{\\beta}$\nof regression coefficients. Rewriting this so that the right hand side\nis $\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta}$, we arrive at a\nnew equation\n$$\\ln\\Big(\\frac{\\operatorname{E}[Y]}{1-\\operatorname{E}[Y]}\\Big)=\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta}.$$\nThe left hand side of this new equation is known as the logit function,\ndefined on the open unit interval $(0,1)$ with range the entire real\nline $\\mathbb{R}$:\n$$\\operatorname{logit}(p):=\\ln(\\frac{p}{1-p})\\mbox{ where }p\\in(0,1).$$\nNote that the logit of $p$ is the same as the natural log of the odds of\nsuccess (over failures) with the probability of success = $p$. Since $Y$\nis a binary response variable, so it has a binomial distribution with\nparameter (probability of success) $p=\\operatorname{E}[Y]$, the logistic\nregression model equation can be rewritten as\n$$\\operatorname{logit}\\big(\\operatorname{E}[Y]\\big)=\\operatorname{logit}(p)=\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta}.$$\n\nLogistic regression is a particular type of generalized linear model. In\naddition, the associated logit function is the most appropriate and\nnatural choice for a link function. By natural we mean that\n$\\operatorname{logit}(p)$ is equal to the natural parameter $\\theta$\nappearing in the distribution function for the GLM (generalized linear\nmodel). To see this, first note that the distribution function for a\nbinomial random variable $Y$ is\n$$P(Y=y)=\\left({n\\atop y}\\right)p^y(1-p)^{(n-y)},$$ where $n$ is the\nnumber of trials and $Y=y$ is the event that there are $y$ success in\nthese $n$ trials. $p$, the parameter, is the probability of success. Let\nthere be $N$ iid binomial random variables $Y_1,Y_2,\\ldots,Y_N$ each\ncorresponding to $n_i$ trials with $p_i$ probability of success. Then\nthe joint probability distribution of these $N$ random variables is\nsimply the product of the individual binomial distributions. Equating\nthis to the distribution for the GLM, which belongs to the exponential\nfamily of distributions, we have:\n$$\\prod_{i=1}^{N}\\left({n_i\\atop y_i}\\right){p_i}^{y_i}(1-p_i)^{(n_i-y_i)} =\\prod_{i=1}^{N}\\operatorname{exp}\\big[y_i\\theta_i-b(\\theta_i)+c(y_i)\\big].$$\nTaking the natural log on both sides, we have the equality of\nlog-likelihood function in two different forms:\n$$\\sum_{i=1}^{N}\\big[\\ln\\left({n_i\\atop y_i}\\right)+y_i\\ln p_i+(n_i-y_i) \\ln(1-p_i)\\big]=\\sum_{i=1}^{N}\\big[y_i\\theta_i-b(\\theta_i)+c(y_i)\\big].$$\nRearranging the left hand side and comparing term $i$, we have\n$$y_i\\ln(\\frac{p_i}{1-p_i})+n_i\\ln(1-p_i)+\\ln\\left({n_i\\atop y_i}\\right)=y_i\\theta_i-b(\\theta_i)+c(y_i),$$\nso that $\\theta_i=\\ln\\big(p_i/(1-p_i)\\big)=\\operatorname{logit}(p_i)$.\n\nNext, setting the natural link function logit of the expected value of\n$Y_i$, which is $p_i$, to the linear portion of the GLM, we have\n$$\\operatorname{logit}(p_i)={\\boldsymbol{X}_i}^{\\operatorname{T}}\\boldsymbol{\\beta},$$\ngiving us the model formula for the logistic regression.\n\n**Remarks.**\n\n-   Comparing model equation for the logistic regression to that of the\n    normal or Gaussian linear regression model, we see that the\n    difference is in the choice of link function. In normal liner model,\n    the regression equation looks like\n    $$\\operatorname{E}[Y]=\\boldsymbol{X}^{\\operatorname{T}}\\boldsymbol{\\beta}.$$\n    The link function in this case is the identity function. The model\n    equation is consistent because the linear terms on the right hand\n    side allow $\\operatorname{E}[Y]$ on the left hand side to vary over\n    the reals. However, for a binary response variable, Equation (2)\n    would not be appropriate as the left hand side is restricted to only\n    within the unit interval, whereas the right hand side has the\n    possibility of going outside of $(0,1)$. Therefore, Equation (1) is\n    more appropriate when we are dealing with a binary response data\n    variable.\n\n-   The logit function is not the only choice of link function for the\n    logistic regression. Other, \u201cnon-natural\u201d link functions are\n    available. Two such examples are the probit function, or the inverse\n    cumulative normal distribution function $\\Phi^{-1}(p)$ and the\n    complimentary-log-log function $\\ln(-\\ln(1-p))$. Both of these\n    functions map the open unit interval to $\\mathbb{R}$.",
  "language": "INFORMAL",
  "phrase": "Logistic Regression",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/LogisticRegression"
    }
  ],
  "indexable": true
}