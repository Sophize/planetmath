{
  "alternatePhrases": [
    "likelihood statistic",
    "likelihood"
  ],
  "definition": "Let **X**=($X_1,\\ldots,X_n$) be a random vector and\n$$\\lbrace f_{\\mathbf{X}}(\\boldsymbol{x}\\mid\\boldsymbol{\\theta}) : \\boldsymbol{\\theta} \\in \\Theta \\rbrace$$\na statistical model parametrized by\n$\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_k)$, the parameter vector\nin the *parameter space* $\\Theta$. The *likelihood function* is a map\n$L: \\Theta \\to \\mathbb{R}$ given by\n$$L(\\boldsymbol{\\theta}\\mid\\boldsymbol{x}) =  f_{\\mathbf{X}}(\\boldsymbol{x}\\mid\\boldsymbol{\\theta}).$$\nIn other words, the likelikhood function is functionally the same in\nform as a probability density function. However, the emphasis is changed\nfrom the $\\boldsymbol{x}$ to the $\\boldsymbol{\\theta}$. The pdf is a\nfunction of the $x$\u2019s while holding the parameters $\\theta$\u2019s constant,\n$L$ is a function of the parameters $\\theta$\u2019s, while holding the $x$\u2019s\nconstant.\n\nWhen there is no confusion, $L(\\boldsymbol{\\theta}\\mid\\boldsymbol{x})$\nis abbreviated to be $L(\\boldsymbol{\\theta})$.\n\nThe parameter vector $\\hat{\\boldsymbol{\\theta}}$ such that\n$L(\\hat{\\boldsymbol{\\theta}})\\geq L(\\boldsymbol{\\theta})$ for all\n$\\boldsymbol{\\theta}\\in\\Theta$ is called a *maximum likelihood\nestimate*, or *MLE*, of $\\boldsymbol{\\theta}$.\n\nMany of the density functions are exponential in nature, it is therefore\neasier to compute the MLE of a likelihood function $L$ by finding the\nmaximum of the natural log of $L$, known as the log-likelihood function:\n$$\\ell(\\boldsymbol{\\theta}\\mid\\boldsymbol{x}) =  \\operatorname{ln}(L(\\boldsymbol{\\theta}\\mid\\boldsymbol{x}))$$\ndue to the monotonicity of the log function.\n\n**Examples**:\n\n1.  A coin is tossed $n$ times and $m$ heads are observed. Assume that\n    the probability of a head after one toss is $\\pi$. What is the MLE\n    of $\\pi$?\n\n    *Solution*: Define the outcome of a toss be 0 if a tail is observed\n    and 1 if a head is observed. Next, let $X_i$ be the outcome of the\n    $i$th toss. For any single toss, the density function is\n    $\\pi^x(1-\\pi)^{1-x}$ where $x\\in \\lbrace 0,1\\rbrace$. Assume that\n    the tosses are independent events, then the joint probability\n    density is\n    $$f_{\\mathbf{X}}(\\boldsymbol{x}\\mid\\pi)=\\binom{n}{\\Sigma x_i}\\pi^{\\Sigma x_i}(1-\\pi)^{\\Sigma (1-x_i)}=\\binom{n}{m}\\pi^m(1-\\pi)^{n-m},$$\n    which is also the likelihood function $L(\\pi)$. Therefore, the\n    log-likelihood function has the form\n    $$\\ell(\\pi\\mid\\boldsymbol{x})=\\ell(\\pi)=\\operatorname{ln}\\binom{n}{m}+m\\operatorname{ln}(\\pi)+(n-m)\\operatorname{ln}(1-\\pi).$$\n    Using standard calculus, we get that the MLE of $\\pi$ is\n    $$\\hat{\\pi}=\\frac{m}{n}=\\overline{x}.$$\n\n2.  Suppose a sample of $n$ data points $X_i$ are collected. Assume that\n    the $X_i\\sim N(\\mu,\\sigma^2)$ and the $X_i$\u2019s are independent of\n    each other. What is the MLE of the parameter vector\n    $\\boldsymbol{\\theta}=(\\mu,\\sigma^2)$?\n\n    *Solution*: The joint pdf of the $X_i$, and hence the likelihood\n    function, is\n    $$L(\\boldsymbol{\\theta}\\mid\\boldsymbol{x})=\\frac{1}{\\sigma^n(2\\pi)^{n/2}}\\operatorname{exp}(-\\frac{\\Sigma(x_i-\\mu)^2}{2\\sigma^2}).$$\n    The log-likelihood function is\n    $$\\ell(\\boldsymbol{\\theta}\\mid\\boldsymbol{x})=-\\frac{\\Sigma(x_i-\\mu)^2}{2\\sigma^2}-\\frac{n}{2}\\operatorname{ln}(\\sigma^2)-\\frac{n}{2}\\operatorname{ln}(2\\pi).$$\n    Taking the first derivative (gradient), we get\n    $$\\frac{\\partial\\ell}{\\partial \\boldsymbol{\\theta}}=(\\frac{\\Sigma(x_i-\\mu)}{\\sigma^2},\\frac{\\Sigma(x_i-\\mu)^2}{2\\sigma^4}-\\frac{n}{2\\sigma^2}).$$\n    Setting\n    $$\\frac{\\partial\\ell}{\\partial \\boldsymbol{\\theta}}=\\boldsymbol{0}\\mbox{ See score function}$$\n    and solve for $\\boldsymbol{\\theta}=(\\mu,\\sigma^2)$ we have\n    $$\\boldsymbol{\\hat{\\theta}}=(\\hat{\\mu},\\hat{\\sigma}^2)=(\\overline{x},\\frac{n-1}{n}s^2),$$\n    where $\\overline{x}=\\Sigma x_i/n$ is the sample mean and\n    $s^2=\\Sigma (x_i-\\overline{x})^2/(n-1)$ is the sample variance.\n    Finally, we verify that $\\hat{\\boldsymbol{\\theta}}$ is indeed the\n    MLE of $\\boldsymbol{\\theta}$ by checking the negativity of the 2nd\n    derivatives (for each parameter).",
  "language": "INFORMAL",
  "phrase": "Likelihood Function",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/LikelihoodFunction"
    }
  ],
  "indexable": true
}