{
  "alternatePhrases": [
    "systematic error"
  ],
  "definition": "**Background**. In estimating a parameter from a statistical model, one\nis interested in how the estimates deviate from the true value of the\nparameter. The deviations generally come from two sources. One source is\nknown as the *noise*, or *random error*, which has to do with the random\nnature of observations or measurements in general. For example, when a\nfair coin is tossed 100 times and the number of heads is counted. One\nmight get 51 even though the true parameter is 50. The difference of 1\nis due to the random nature of coin tossing.\n\nThe other source of deviation is known as the *bias*, or *systematic\nerror*, which has to do with how the observations are made, how the\ninstruments are set up to make the measurements, and most of all, how\nthese observations or measurements are tallied and summarized to come up\nwith an estimate of the true parameter. For example, a rating scheme is\nproposed for an online collaborative encyclopedia on entries contributed\nby individuals who are members of the online website hosting the\nencyclopedia. The purpose of this rating scheme is to give the readers,\nmembers or non-members inclusive, a better idea on the quality of the\nentries by their corresponding numerical values. Suppose that members\nare asked to rate an entry from a scale of 1 to 10. For simplicity,\nmembers who are intimately familiar with the concept in the entry rate\nit with a perfect 10. Next, members who are not that familiar with the\nentry give it a 5. Finally, the remaining members choose to not\nparticipate and the rating scale from them default to a 0. A simple\narithmetic average is computed and a rating of 2.5 is produced. Would\nthis 2.5 be a good indicator of the overall quality of the entry? Maybe\nnot. Here, biases are introduced. First, the participants of the rating\nscheme do not include non-members, who, collectively, may very well\nrepresent a different level of understanding of the rated entry than\nmembers. Secondly, even among the members, there is a considerable\namount of differences in terms of levels of understanding of the entry,\netc... To some, the entry may be accurately and perfectly written, not\neveryone will rate it the same way in the end. Finally, there are the\nnon-raters. We have no idea as to how they would rate the entries. Their\nvotes should certainly count if they decide to rate in the last minute.\nThe final rate, however, would most likely be different.\n\nThe difference between the bias and the noise is that one can be reduced\nwhile the other can not. Mathematically, we have the following:\n\n**Definition**. If $\\theta$ is a parameter in a statistical model, the\nbias of an estimator $\\hat{\\theta}$ of $\\theta$, is the difference\nbetween expectation of $\\hat{\\theta}$ and the value of $\\theta$, which,\nby abuse of notation, is also denoted $\\theta$:\n$$\\operatorname{Bias}(\\hat{\\theta}):=\\operatorname{E}[\\hat{\\theta}]-\\theta.$$\nAn estimator is called an *unbiased estimator* if its bias is zero at\n*all* values of $\\theta$. Otherwise, it is a *biased estimator*.\n\nNote that the random error does not appear in the above definition\nbecause its expectation is zero.\n\n**Examples**.\n\n1.  If observations $X_1,\\ldots,X_n$ are iid from a normal distribution\n    with mean $\\mu$ and variance $\\sigma^2$, then the sample mean\n    estimator $\\overline{X}$ is an unbiased estimator for $\\mu$. To see\n    this, recall the definition of a sample mean\n    $$\\overline{X}=\\frac{1}{n}(X_1+\\cdots+X_n)$$ so that\n    $$\\operatorname{E}[\\overline{X}]=\\frac{1}{n}(\\operatorname{E}[X_1]+ \\cdots+\\operatorname{E}[X_n]).$$\n    But $\\mu=\\operatorname{E}[X_1]=\\cdots=\\operatorname{E}[X_n]$, the\n    above expression reduces to\n    $$\\operatorname{E}[\\overline{X}]=\\frac{1}{n}(n\\mu)=\\mu,$$ showing\n    that the bias of $\\overline{X}$ is zero. Note that even though\n    $\\overline{X}$ depends on the size of the sample $n$, its\n    expectation, however, does not, and is identically $\\mu$, for all\n    values of $\\mu$.\n\n2.  Here is another example of an unbiased estimator. Again, assume\n    observations $X_i$, $i=1,\\ldots,n$ are iid as normal distribution\n    $N(\\mu,\\sigma^2)$. The sample variance estimator $s^2$ is defined by\n    $$s^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2.$$ Expressing\n    $s^2$ explicitly in terms of the random variables $X_i$, we have\n    $$\\begin{aligned}\n    (n-1)s^2 &=& \\sum_{i=1}^{n}X_i^2-\\frac{1}{n}(\\sum_{i=1}^{n}X_i)^2 \\\\ \n    &=& \\frac{1}{n}[(n-1)\\sum_{i=1}^{n}X_i^2-2\\sum_{i<j}X_iX_j] \\end{aligned}$$\n    Now, for $i\\neq j$, $X_i$ and $X_j$ are independent so that\n    $$\\operatorname{E}[X_iX_j]=\\operatorname{E}[X_i]\\operatorname{E}[X_j]=\\mu^2\n    =\\operatorname{E}[X_i]^2,$$ for all $i=1,\\ldots,n$. Hence\n    $$\\begin{aligned}\n    (n-1)\\operatorname{E}[s^2] &=&\n    \\frac{1}{n}\\lbrace(n-1)\\sum_{i=1}^{n}\\operatorname{E}[X_i^2]- 2\\sum_{i<j}\\operatorname{E}[X_iX_j]\\rbrace \\\\\n    &=& \\frac{1}{n}\\lbrace(n-1)\\sum_{i=1}^{n}\\operatorname{E}[X_i^2]-2\\sum_{i<j}\\mu^2\\rbrace \\\\\n    &=& \\frac{1}{n}\\lbrace(n-1)\\sum_{i=1}^{n}\\operatorname{E}[X_i^2]-2\\cdot\\frac{n(n-1)}{2}\\mu^2 \\rbrace \\\\\n    &=& \\frac{n-1}{n}\\sum_{i=1}^{n}\\lbrace\\operatorname{E}[X_i^2]-\\mu^2\\rbrace \\\\\n    &=& \\frac{n-1}{n}\\sum_{i=1}^{n}\\lbrace \\operatorname{E}[X_i^2]-\\operatorname{E}[X_i]^2\\rbrace \\\\\n    &=& \\frac{n-1}{n}\\sum_{i=1}^{n}\\operatorname{Var}[X_i]=(n-1)\\sigma^2.\\end{aligned}$$\n    This shows that $s^2$ is an unbiased estimator for $\\sigma^2$.\n\n3.  However, $s^2$ would be biased if we were to define it by\n    $$s^2=\\frac{1}{n}\\sum_{i=1}^{n}(X_i-\\overline{X})^2,$$ since\n    $$\\operatorname{E}[s^2]=\\frac{n-1}{n}\\sigma^2$$ would depend on the\n    sample size $n$ and would not equal to $\\sigma^2$ at any $n$.\n\n**Remark**. In practice, unbiased estimators are rare. There is another,\nlarger class of estimators that are biased with smaller samples, but the\nbias gets smaller and tends to 0 as the sample size gets larger. Such an\nestimator is called an *asymptotically unbiased estimator*. For example,\nif we were to define $s^2$ as in Example 3 above, $s^2$ would be an\nasymptotically unbiased estimator for $\\sigma^2$.",
  "language": "INFORMAL",
  "phrase": "Bias",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/Bias"
    }
  ],
  "indexable": true
}