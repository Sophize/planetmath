{
  "alternatePhrases": [
    "entropy",
    "Shannon entropy"
  ],
  "definition": "#### Definition (Discrete)\n\nLet $X$ be a discrete random variable on a finite set\n$\\mathcal{X}=\\{x_1,\\ldots,x_n\\}$, with probability distribution function\n$p(x) = \\Pr(X=x)$. The [*entropy*]{} $H(X)$ of $X$ is defined as\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_b p(x).$$ The convention\n$0 \\log 0 =0$ is adopted in the definition. The logarithm is usually\ntaken to the base 2, in which case the entropy is measured in \u201cbits,\u201d or\nto the base [*e*]{}, in which case $H(X)$ is measured in \u201cnats.\u201d\n\nIf $X$ and $Y$ are random variables on $\\mathcal{X}$ and $\\mathcal{Y}$\nrespectively, the [*joint entropy*]{} of $X$ and $Y$ is\n$$H(X,Y) = -\\sum_{(x,y)\\in \\mathcal{X}\\times \\mathcal{Y}} p(x,y)\n  \\log_b p(x,y),$$ where $p(x,y)$ denote the joint distribution of $X$\nand $Y$.\n\n#### Discussion\n\nThe Shannon entropy was first introduced by Shannon in 1948 in his\nlandmark paper \u201cA Mathematical Theory of Communication.\u201d The entropy is\na functional of the probability distribution function $p(x)$, and is\nsometime written as $$H(p(x_1), p(x_2),\\ldots, p(x_n)).$$ It is noted\nthat the entropy of $X$ does not depend on the actual values of $X$, it\nonly depends on $p(x)$. The definition of Shannon\u2019s entropy can be\nwritten as an expectation $$H(X) = -E[\\log_b p(X)].$$ The quantity\n$-\\log_b p(x)$ is interpreted as the information content of the outcome\n$x\\in\\mathcal{X}$, and is also called the Hartley information of $x$.\nHence the Shannon\u2019s entropy is the average amount of information\ncontained in random variable $X$, it is also the uncertainty removed\nafter the actual outcome of $X$ is revealed.\n\n#### Characterization\n\nWe write $H(X)$ as $H_n(p_1,\\ldots,p_n)$. The Shannon entropy satisfies\nthe following properties.\n\n1.  For any $n$, $H_n(p_1,\\ldots,p_n)$ is a continuous and symmetric\n    function on variables $p_1$, $p_2,\\ldots, p_n$.\n\n2.  Event of probability zero does not contribute to the entropy,\n    i.e.for any $n$,\n    $$H_{n+1}(p_1,\\ldots,p_n,0) = H_n(p_1,\\ldots,p_n).$$\n\n3.  Entropy is maximized when the probability distribution is uniform.\n    For all $n$,\n    $$H_n(p_1,\\ldots,p_n) \\leq H_n\\Big(\\frac{1}{n},\\ldots,\\frac{1}{n}\n    \\Big).$$ This follows from Jensen inequality,\n    $$H(X) = E\\Big[\\log_b \\Big( \\frac{1}{p(X)}\\Big) \\Big] \\leq\n       \\log_b \\Big( E\\Big[ \\frac{1}{p(X)} \\Big] \\Big) = \\log_b(n).$$\n\n4.  If $p_{ij}$, $1\\leq i \\leq m$, $1\\leq j \\leq n$ are non-negative\n    real numbers summing up to one, and $q_i =\n    \\sum_{j=1}^n p_{ij}$, then\n    $$H_{mn}(p_{11},\\ldots, p_{mn}) = H_m(q_1,\\ldots,q_m) +\n     \\sum_{i=1}^m q_i H_n\\Big(\\frac{p_{i1}}{q_i},\\ldots, \\frac{p_{in}}{q_i}\n     \\Big).$$ If we partition the $mn$ outcomes of the random experiment\n    into $m$ groups, each group contains $n$ elements, we can do the\n    experiment in two steps: first determine the group to which the\n    actual outcome belongs to, and second find the outcome in this\n    group. The probability that you will observe group $i$ is $q_i$. The\n    conditional probability distribution function given group $i$ is\n    $(p_{i1}/q_i,\\ldots,p_{in}/q_i)$. The entropy\n    $$H_n\\Big(\\frac{p_{i1}}{q_i},\\ldots, \\frac{p_{in}}{q_i} \\Big)$$ is\n    the entropy of the probability distribution conditioned on group\n    $i$. Property 4 says that the total information is the sum of the\n    information you gain in the first step, $H_m(q_1,\\ldots,\n    q_m)$, and a weighted sum of the entropies conditioned on each\n    group.\n\nKhinchin in 1957 showed that the only function satisfying the above\nassumptions is of the form:\n$$H_n(p_1,\\ldots,p_n) = -k \\sum_{i=1}^n p_i \\log p_i$$ where $k$ is a\npositive constant, essentially a choice of unit of measure.\n\n#### Definition (Continuous)\n\nEntropy in the continuous case is called **.\n\n#### Discussion\u2014Continuous Entropy\n\nDespite its seductively analogous form, continuous entropy cannot be\nobtained as a limiting case of discrete entropy.\n\nWe wish to obtain a generally finite measure as the \u201cbin size\u201d goes to\nzero. In the discrete case, the bin size is the (implicit) width of each\nof the $n$ (finite or infinite) bins/buckets/states whose probabilities\nare the $p_n$. As we generalize to the continuous domain, we must make\nthis width explicit.\n\nTo do this, start with a continuous function $f$ discretized as shown in\nthe figure:\n\n![Discretizing the function $f$ into bins of width\n$\\Delta$](function-with-bins.eps){width=\"\\textwidth\"}\n\nAs the figure indicates, by the mean-value theorem there exists a value\n$x_i$ in each bin such that\n$$f(x_i) \\Delta = \\int_{i\\Delta}^{(i+1)\\Delta} f(x) dx$$ and thus the\nintegral of the function $f$ can be approximated (in the Riemannian\nsense) by\n$$\\int_{-\\infty}^{\\infty} f(x) dx = \\lim_{\\Delta \\to 0} \\sum_{i = -\\infty}^{\\infty} f(x_i) \\Delta$$\nwhere this limit and \u201cbin size goes to zero\u201d are equivalent.\n\nWe will denote\n$$H^{\\Delta} {:=}- \\sum_{i=-\\infty}^{\\infty} \\Delta f(x_i) \\log \\Delta f(x_i)$$\nand expanding the $\\log$ we have $$\\begin{aligned}\nH^{\\Delta} &= - \\sum_{i=-\\infty}^{\\infty} \\Delta f(x_i) \\log \\Delta f(x_i)\\\\\n&= - \\sum_{i=-\\infty}^{\\infty} \\Delta f(x_i) \\log f(x_i) -\\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta \\log \\Delta.\\end{aligned}$$\nAs $\\Delta \\to 0$, we have $$\\begin{aligned}\n\\sum_{i=-\\infty}^{\\infty} f(x_i) \\Delta &\\to \\int f(x) dx = 1\\qquad \\text{and}\\\\\n\\sum_{i=-\\infty}^{\\infty} \\Delta f(x_i) \\log f(x_i) &\\to \\int f(x) \\log f(x) dx\\end{aligned}$$\nThis leads us to our definition of the differential entropy (continuous\nentropy):\n$$h[f] = \\lim_{\\Delta \\to 0} \\left[H^{\\Delta} + \\log \\Delta\\right]  = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x) dx.$$",
  "language": "INFORMAL",
  "phrase": "Shannon'S Entropy",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/ShannonsEntropy"
    }
  ],
  "indexable": true
}