{
  "alternatePhrases": [],
  "definition": "Let $(X, \\mathfrak{B}, \\mu)$ be a probability space, and let\n$f \\in L^p(X, \\mathfrak{B}, \\mu)$, $||f||_{p} = 1$ be a function. The\n*differential entropy* $h(f)$ is defined as\n\n$$h(f) \\equiv -\\int_{X} |f|^p \\log |f|^p\\ d\\mu$$\n\nDifferential entropy is the continuous version of the Shannon entropy,\n$H[{\\mathbf{p}}] = -\\sum_{i} p_i \\log p_i$. Consider first $u_a$, the\nuniform 1-dimensional distribution on $(0,a)$. The differential entropy\nis\n\n$$h(u_a) = -\\int_{0}^{a} \\frac{1}{a} \\log \\frac{1}{a}\\ d\\mu = \\log a.$$\n\nNext consider probability distributions such as the function\n$$g = \\frac{1}{2 \\pi \\sigma}e^{-\\frac{(t-\\mu)^2}{2 \\sigma^2}},$$ the\n1-dimensional Gaussian. This pdf has differential entropy\n\n$$h(g) = -\\int_{\\mathbb{R}} g \\log g\\ dt = \\frac{1}{2} \\log 2 \\pi e \\sigma^2.$$\n\nFor a general $n$-dimensional\n$\\mathcal{N}_{n}({\\mathbf{\\mu}},{\\mathbf{K}})$ with mean vector\n${\\mathbf{\\mu}}$ and covariance matrix ${\\mathbf{K}}$,\n$K_{ij} = {\\mathrm{cov}}(x_i, x_j)$, we have\n\n$$h(\\mathcal{N}_{n}({\\mathbf{\\mu}},{\\mathbf{K}})) = \\frac{1}{2} \\log (2 \\pi e)^n |{\\mathbf{K}}|$$\n\nwhere $|{\\mathbf{K}}| = \\det{{\\mathbf{K}}}$.",
  "language": "INFORMAL",
  "phrase": "Differential Entropy",
  "remarks": "",
  "citations": [
    {
      "textCitation": "https://planetmath.org/DifferentialEntropy"
    }
  ],
  "indexable": true
}